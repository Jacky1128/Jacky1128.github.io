<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Zedong Wang, ÁéãÊ≥ΩÊ†ã, Deep learning, HKUST">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/HKUST.ico">
<title>Zedong Wang</title>
<script async defer src="https://buttons.github.io/buttons.js"></script>
</head>


<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./" style="color:#1240A0"><img src="./Files/Profile_Twitter.jpeg" alt="" height="222px" /></a>&nbsp;</td>
<td align="left"><p><a href="./" style="color:#1240A0"><font size="4">Zedong Wang (</font><font size="4"; font style="font-family:Microsoft YaHei">ÁéãÊæ§Ê£ü; ÁéãÊ≥ΩÊ†ã; Jacky</font><font size="4">)</font></a><br />
<i> <a href="https://en.westlake.edu.cn" target="_blank" style="color:#0e19ee">Westlake University</a>, <a href="https://hkust.edu.hk" target="_blank" style="color:#0e19ee">The Hong Kong University of Science and Technology (HKUST)</a></i>
<br /><br />
<a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Stan Z. Li's AI Lab@WestlakeU</a>, <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Dan Xu's Vision Group@HKUST</a><br />
<br />
Location: E2 Building, Westlake University, Dunyu Road #600, Xihu District, Hangzhou, Zhejiang, China.<br />

<!--
Location: E2-223, Westlake University, Dunyu Road #600, Xihu District, Hangzhou, Zhejiang, China.<br />
Location: Building 2-Room 508, Shilongshan Street #18, Xihu District, Hangzhou, Zhejiang, China<br /> -->
<class="staffshortcut">
 <A HREF="#News" style="color:#1240A0">News</A> | 
 <A HREF="#Interests" style="color:#1240A0">Interests</A> | 
 <A HREF="#Education" style="color:#1240A0">Education</A> | 
 <A HREF="#Internship" style="color:#1240A0">Internship</A> | 
 <A HREF="#Publications" style="color:#1240A0">Publications</A> | 
 <A HREF="#Services" style="color:#1240A0">Services</A> | 
 <A HREF="#Awards" style="color:#1240A0">Honors</A> | 
 <A HREF="#Acknowledgement" style="color:#1240A0">Acknowledgement</A> | 
 <A HREF="#Fragments of Memories" style="color:#1240A0">Memories</A>  
 <!--<A HREF="./Files/ZedongWang_CV.pdf" style="color:#1240A0">CV</A> | 
 <A HREF="#Awards" style="color:#2a7ce0">Awards</A> -->
<br />
<br />

Email: jackywang28@outlook.com (prior); wangzedong@westlake.edu.cn<br />
[<a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#1240A0">Google Scholar</a>]  
[<a href="https://www.semanticscholar.org/author/Zedong-Wang/2184760529" target="_blank" style="color:#1240A0">Semantic Scholar</a>] 
[<a href="https://openreview.net/profile?id=~Zedong_Wang1" target="_blank" style="color:#1240A0">OpenReview</a>] 
[<a href="https://twitter.com/ZedongWangAI" target="_blank" style="color:#1240A0">Twitter (X)</a>]
[<a href="https://huggingface.co/ZedongWangAI" target="_blank" style="color:#1240A0">ü§óHF</a>]
[<a href="https://github.com/Jacky1128" target="_blank" style="color:#1240A0">GitHub</a>] 
[<a href="https://orcid.org/0009-0000-0112-0491" target="_blank" style="color:#1240A0">ORCID</a>] 
[<a href="https://dblp.org/pid/179/8811.html" target="_blank" style="color:#1240A0">DBLP</a>] 

<br />
<br />
<!--
<i>‚ÄúThe true value of a man is not determined by his possession, supposed or real, of Truth, but rather by his sincere exertion to get to the Truth. It is not possession of the Truth, but rather the pursuit of Truth by which he extends his powers and in which his ever-growing perfectibility is to be found.‚Äù</i><br />
-->
<i>I am looking to long-term collaboration on related research topics, such as Multi-Task Learning, Multi-modal Learning, Efficient Neural Network Architectures, and more. Please feel free to contact me!</i><br />
</td></tr></table>

<!-- 
<A NAME="About Me"><h2>About Me</h2></A>
<div style="text-align:justify; line-height:120%">
Greetings! I am Zedong Wang, a <a href="https://en.wikipedia.org/wiki/Hong_Kong" target="_blank" style="color:#1240A0">Hong KongK</a>-born AI researcher. 
I am currently a research intern at <a href="https://hkust.edu.hk" target="_blank" style="color:#1240A0">The Hong Kong University of Science and Technology (HKUST)</a>, 
advised by <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Prof. Dan Xu</a>. 
I am also a research intern at <a href="https://en.wikipedia.org/wiki/Zeekr" target="_blank" style="color:#1240A0">ZEEKR, Geely Holding Group.</a>
I was a visiting student at <a href="https://en.westlake.edu.cn" target="_blank" style="color:#1240A0">Westlake University</a>, 
under the supervision of <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Chair Prof. Stan Z. Li (IEEE Fellow, IAPR Fellow)</a>.
I obtained my B.Eng. degree in Electronics and Information Engineering at <a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#1240A0">Huazhong University of Science and Technology (HUST)</a> in 2023, 
advised by <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a>.
I have also been a research intern in <a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#1240A0"> SIAT-MMLab</a> led by Prof. Yu Qiao, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences.
</p>
My research interests center around Computer Vision and Multi-Task Learning.
My overarching goal is to advance principled multi-task multi-modal representation learning that can benefit a wide range of real-world applications. 
</div>
-->



<A NAME="News"><h2>News</h2></A>
  <!-- <div style="height:200px;overflow-y:auto;background:#ffffff;"> -->
  <div style="height:260px;overflow-y:auto;">
  <ul>
    <li><b> <font color="#9F7320">[2024.11.04]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> for <b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC 2024 (<font color="#FF0000">rate: 19.3%, 166/860</font>)</a></b>. Happy to contribute to the community! Huge thanks to the BMVC organizers for their dedication.</li></li>
    <li><b> <font color="#9F7320">[2024.11.04]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> for <b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM 2024 (<font color="#FF0000">rate: 139/X</font>)</a></b>. Happy to contribute to the community! Huge thanks to the ACM MM organizers for their dedication.</li></li>
    <li><b> <font color="#9F7320">[2024.11.04]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://cvpr.thecvf.com/Conferences/2025" target="_blank" style="color:#000000">CVPR 2025</a></b>, which will be held in Nashville TN, USA.</li></li>
    <li><b> <font color="#9F7320">[2024.10.10]</font> </b> One paper on <i>vision backbones & optimizers</i>, <i><a href="https://arxiv.org/abs/2410.06373" target="_blank" style="color:#0e19ee"> BOCB</a></i>, is released. Welcome to visit the <a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#1240A0">HF</a> and <a href="https://bocb-ai.github.io" target="_blank" style="color:#1240A0">Project</a> page for further discussion! Happy to explore something interesting together with <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#1240A0">Juanxi Tian</a> and <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#1240A0">Siyuan Li</a>!</li></li>
    <li><b> <font color="#9F7320">[2024.09.24]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> for <b><a href="https://eccv.ecva.net/Conferences/2024" target="_blank" style="color:#000000">ECCV 2024 (<font color="#FF0000">rate: 2.7%, 198/7293</font>)</a></b>. Happy to contribute to the community! Huge thanks to the ECCV organizers for their dedication.</li></li>
    <li><b> <font color="#9F7320">[2024.08.15]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://iclr.cc/Conferences/2025" target="_blank" style="color:#000000">ICLR 2025</a></b>, which will be held in Singapore.</li></li>
    <li><b> <font color="#9F7320">[2024.07.20]</font> </b> I am invited to serve as <i>Program Committee Member</i> for <b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI 2025</a></b>, which will be held in Philadelphia, USA. </li></li>
    <li><b> <font color="#9F7320">[2024.07.14]</font> </b> Maintain an open-source repository on <i>optimizers</i>, <i><a href="https://github.com/tianshijing/Awesome-Optimizers" target="_blank" style="color:#0e19ee">Awesome-Optimizers</a></i>, with <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#1240A0">Juanxi Tian</a> and <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#1240A0">Siyuan Li</a>. </li>
    <li><b> <font color="#9F7320">[2024.06.04]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://neurips.cc/Conferences/2024/CallForDatasetsBenchmarks" target="_blank" style="color:#000000">NeurIPS 2024 (Datasets and Benchmarks Track)</a></b>. </li></li>
    <li><b> <font color="#9F7320">[2024.05.02]</font> </b> One paper on <i>AI4Science (genomics)</i>, <i><a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#0e19ee"> VQDNA</a></i>, is accepted at <b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML 2024</a></b>. </li></li>
    <li><b> <font color="#9F7320">[2024.05.02]</font> </b> Co-authored paper on <i>long sequence modeling</i>, <i><a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#0e19ee"> CHELA</a></i>, is accepted at <b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#1240A0">Zicheng Liu</a>! </li></li>
    <li><b> <font color="#9F7320">[2024.05.01]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC 2024</a></b>, which will be held in Glasgow, UK.</li></li>
    <li><b> <font color="#9F7320">[2024.04.16]</font> </b> Co-authored paper on <i>long sequence modeling</i>, <i><a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#0e19ee"> LongVQ</a></i>, is accepted at <b><a href="https://ijcai24.org" target="_blank" style="color:#000000">IJCAI 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#1240A0">Zicheng Liu</a>! </li></li>
    <li><b> <font color="#9F7320">[2024.04.13]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM 2024</a></b>, which will be held in Melbourne, Australia. </li></li>
    <li><b> <font color="#9F7320">[2024.03.12]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://eccv.ecva.net/Conferences/2024" target="_blank" style="color:#000000">ECCV 2024</a></b>, which will be held in MiCo Milano, Italy.</li></li>
    <li><b> <font color="#9F7320">[2024.01.22]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML 2024</a></b>, which will be held in Vienna, Austria.</li></li>
    <li><b> <font color="#9F7320">[2024.01.17]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://icpr2024.org" target="_blank" style="color:#000000">ICPR 2024</a></b>, which will be held in Kolkata, India.</li></li>
    <li><b> <font color="#9F7320">[2024.01.16]</font> </b> One paper on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#0e19ee"> MogaNet</a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#1240A0">Code & weights</a> (180 stars) are released!</li></li>
    <li><b> <font color="#9F7320">[2024.01.16]</font> </b> Co-authored paper on <i>semi-sup learning</i>, <i><a href="https://arxiv.org/abs/2310.03013" target="_blank" style="color:#0e19ee"> SemiReward</a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#1240A0">Siyuan Li</a>! </li></li>
    <li><b> <font color="#9F7320">[2024.01.09]</font> </b> I am invited to serve as an <i> emergency reviewer</i> for <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b> <b><a href="https://iclr.cc/Conferences/2024/CallForTinyPapers" target="_blank" style="color:#000000">(TinyPapers)</a></b>. It will be held in Vienna, Austria. </li></li>
    <li><b> <font color="#9F7320">[2023.12.31]</font> </b> Co-authored preprint on <i>self-supervised learning</i>, <i><a href="https://arxiv.org/abs/2401.00897" target="_blank" style="color:#0e19ee"> Masked Modeling on Vision and Beyond</a></i>. </li></li>
    <li><b> <font color="#9F7320">[2023.09.31]</font> </b> Co-authored paper on <i>video prediction</i>, <i><a href="https://arxiv.org/abs/2306.11249" target="_blank" style="color:#0e19ee"> OpenSTL</a></i>, is accepted at <b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">NeurIPS 2023</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=6kTV6aMAAAAJ" target="_blank" style="color:#1240A0">Cheng Tan</a>! </li></li>
    <li><b> <font color="#9F7320">[2023.06.25]</font> </b> Got my B.Eng. degree from Huazhong University of Science and Technology (HUST)! Special thanks to my undergraduate supervisor <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a> for the generous support!</li>
    <li><b> <font color="#9F7320">[2023.05.23]</font> </b> One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2111.15454" target="_blank" style="color:#0e19ee"> SAMix</a></i>, is presented for both SL & SSL scenarios. </li>
    <li><b> <font color="#9F7320">[2022.11.07]</font> </b> One preprint on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#0e19ee"> MogaNet</a></i>. A new family of pure convolutional architecture covering <b>5M~100M+</b> model scales with great performance.  <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#1240A0">Code & weights</a> are released (180 stars). Welcome to discuss, use, and star!</li>
    <li><b> <font color="#9F7320">[2022.09.11]</font> </b> One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2209.04851" target="_blank" style="color:#0e19ee">OpenMixup</a></i>, is presented for vision tasks. This is also my first arXiv paper!</li>
    <li><b> <font color="#9F7320">[2022.09.11]</font> </b> Maintain an open-source repository, <i><a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#0e19ee">OpenMixup</a></i> (618 stars), for both supervised, semi- and self-supervised visual representation learning based on PyTorch. On updating! </li>
    <li><b> <font color="#9F7320">[2022.07.06]</font> </b> Fortunate to become <i>visiting student</i> at Westlake University, under the supervision of <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Chair Prof. Stan Z. Li</a>. </li></li>
    <li><b> <font color="#9F7320">[2021.09.01]</font> </b> Fortunate to become <i>research intern</i> in <a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#1240A0"> HUST Vision Lab</a>, under the supervision of <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a>. </li></li>
    <li><b> <font color="#9F7320">[2021.06.01]</font> </b> Fortunate to become <i>research intern</i> in <a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#1240A0"> SIAT-MMLab</a> at Shenzhen Institute of Advanced Technology, CAS. </li></li>

</ul>
</div>


<A NAME="Interests"><h2>Research Interests</h2></A>
Currently, I focus mostly on Multi-Task Learning and Multi-modal Learning, including (but not limited to):
<ul>
    <li>Label-Efficient Learning: Mixup Augmentation [<a href="https://arxiv.org/abs/2209.04851" target="_blank" style="color:#1240A0">OpenMixup</a>, <a href="https://arxiv.org/abs/2111.15454" target="_blank" style="color:#1240A0">SAMix</a>], Semi-supervised Learning [<a href="https://arxiv.org/abs/2310.03013" target="_blank" style="color:#1240A0">SemiReward</a>].</li>
    <li>Neural Network Architectures: Vision Backbones [<a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#1240A0">MogaNet</a>], Efficient Multi-Task Architectures.</li>
    <li>Optimization & Regularization: Deep Learning Optimizers [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#1240A0">SEMA</a>, <a target="_blank" style="color:#1240A0">BOCB</a>], Multi-Task Optimization.</li>
    <li>Downstream Applications: AI4Science [<a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#1240A0">VQDNA</a>], Autonomous Driving.</li>
</ul>
<br />


<A NAME="Education"><h2>Education</h2></A>
<!--
<ul>
<li>2019.09-2023.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B.Eng. at <a href="https://www.hust.edu.cn" target="_blank" style="color:#1240A0">Huazhong University of Science and Technology</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a>.</li>
</ul>
<br />
-->
<p style="line-height: 0.1;">&nbsp; </p>

<p style="margin-left: 1em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="66"/>
    &nbsp&nbsp&nbsp&nbsp <b><a href="https://www.hust.edu.cn" target="_blank" style="color:#000000">Huazhong University of Science and Technology</a> (2019-2023) </b><br> 
    &nbsp&nbsp&nbsp&nbsp &bull; B.Eng. in Electronic and Information Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Thesis:</i> Efficient ConvNet-based Vision Backbone (92/100, with full score on NOVELTY term). <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. <b>Xinggang Wang</b></a>.<br>
</p>
<p style="line-height: 0.1;">&nbsp; </p>

<A NAME="Internship"><h2>Research Experience</h2></A>
<!--
<ul>
<li>2024.04-now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on multi-task perception at <a href="https://en.wikipedia.org/wiki/Zeekr" target="_blank" style="color:#1240A0">ZEEKR, Geely Holding Group</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Prof. Dan Xu</a>.</li>
<li>2024.03-now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp;&thinsp; Research on multi-modal & multi-task learning at <a href="https://hkust.edu.hk" target="_blank" style="color:#1240A0">HKUST</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp;&thinsp;&nbsp;&nbsp;&thinsp;&thinsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Prof. Dan Xu</a>.</li>
<li>2022.06-now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp;&thinsp; Research on representation learning & AI4S at <a href="https://en.westlake.edu.cn" target="_blank" style="color:#1240A0">Westlake University</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Chair Prof. Stan Z. Li</a>.</li>
<li>2021.09-2023.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on few-shot semantic segmentation at <a href="https://www.hust.edu.cn" target="_blank" style="color:#1240A0">HUST Vision Lab</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a>.</li>
<li>2021.06-2021.09 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp; Research on semantic segmentation & text spotting at <a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#1240A0">SIAT-MMLab</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#1240A0">Dr. Bin Fu</a>.</li>
<li>2020.06-2021.04 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on remote sensing high-res semantic segmentation at <a href="http://www.digitalearthlab.com.cn" target="_blank" style="color:#1240A0">CAS</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=TpX2C3cAAAAJ" target="_blank" style="color:#1240A0">Dr. Xiaoping Du</a>.</li>
</ul>
<br /> 
-->
<p style="line-height: 0.1;">&nbsp; </p>

<p style="margin-left: 1.7em;"> 
    <img src="Files/ZEEKR_logo.png" align="left" width="65" height="65"/>
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://en.wikipedia.org/wiki/Zeekr" target="_blank" style="color:#000000">ZEEKR Intelligent Technology, Geely</a> (2024.04-present) </b><br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Multi-Task Learning in Autonomous Driving. <br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Prof. <b>Dan Xu</b></a> (University-Enterprise Cooperation). <br>
</p>

<p style="line-height: 0.3;">&nbsp; </p>

<p style="margin-left: 2.1em;"> 
    <img src="Files/HKUST.ico" align="left" width="54" height="66"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&thinsp; <b><a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#000000">Dan Xu's Vision Group, CSE, HKUST</a> (2024.03-present) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&thinsp; &bull; <i>Topics:</i> Efficient Multi-Task Learning. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&thinsp; &bull; <i>Advisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Prof. <b>Dan Xu</b></a>.<br>
</p>

<p style="line-height: 0.3;">&nbsp; </p>

<p style="margin-left: 1.9em;"> 
    <img src="Files/westlake.ico" align="left" width="62" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#000000">Stan Z. Li's AI Lab, Westlake University</a> (2022.06-present) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Representation Learning and AI for Life Science (Genomics). <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Chair Prof. <b>Stan Z. Li</b> (IEEE Fellow, IAPR Fellow)</a>.<br>
</p>

<p style="line-height: 0.3;">&nbsp; </p>

<p style="margin-left: 1.2em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="65"/>
    &nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#000000">Xinggang Wang's Vision Group, HUST</a> (2021.09-2022.06) </b><br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Computer Vision and Few-shot Semantic Segmentation. <br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. <b>Xinggang Wang</b></a>.<br>
</p>

<p style="line-height: 0.3;">&nbsp; </p>

<p style="margin-left: 1.9em;"> 
    <img src="Files/SIAT_logo.png" align="left" width="64" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; <b><a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#000000">SIAT-MMLab, Chinese Academy of Sciences</a> (2021.06-2021.09) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Topics:</i> Semantic Segmentation and Text Spotting. <br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Advisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#1240A0">Dr. <b>Bin Fu</b></a>.<br>
</p>

<p style="line-height: 0.2;">&nbsp; </p>

<A NAME="Publications"><h2>Publications</h2></A>

<p><b>Selected Preprints (*: Equal Contribution. ‚Ä†: Corresponding Author.)</b>: </p>
<font size="3"> 
<ul>
<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_BOCB.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a target="_blank" style="color:#FF0000">[NEW!]</a></b> <b><a href= "https://huggingface.co/papers/2410.06373" target="_blank" style="color:#1240A0">Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning</a></b></font><br>
        <i> Siyuan Li*, Juanxi Tian*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Luyuan Zhang, Zicheng Liu, Weiyang Jin, Yang Liu, Baigui Sun, Stan Z. Li‚Ä† </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2410.06373" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#1240A0">ü§óHF</a>]
        [<a href="https://bocb-ai.github.io" target="_blank" style="color:#1240A0">Project</a>]
        [<a href="https://github.com/Black-Box-Optimization-Coupling-Bias/BOCB" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_SEMA.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://huggingface.co/papers/2402.09240" target="_blank" style="color:#1240A0">Switch EMA: A Free Lunch for Better Flatness and Sharpness</a></b></font><br>
        <i> Siyuan Li*, Zicheng Liu*, Juanxi Tian*, Ge Wang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Li‚Ä† </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://huggingface.co/papers/2402.09240" target="_blank" style="color:#1240A0">ü§óHF</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2023_MM_Survey.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#1240A0">Masked Modeling for Self-supervised Representation Learning on Vision and Beyond</a></b></font><br>
        <i> Siyuan Li*, Luyuan Zhang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Di Wu, Lirong Wu, Zicheng Liu, Jun Xia, Cheng Tan, Yang Liu, Baigui Sun, Stan Z. Li‚Ä† </a></i><br><i><b>arXiv, 2023</b></i><br>
        [<a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Lupin1998/Awesome-MIM" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/arXiv_2023_MIMSurvey_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2022_OpenMixup.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2209.04851" target="_blank" style="color:#1240A0">OpenMixup: A Comprehensive Mixup Benchmark for Visual Classification</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Zicheng Liu, Di Wu, Cheng Tan, Weiyang Jin, Stan Z. Li‚Ä† </a></i><br><i><b>arXiv, 2022</b></i><br>
        [<a href= "https://arxiv.org/abs/2209.04851" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/arxiv_2022_OpenMixup_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv22_SAMix.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2111.15454" target="_blank" style="color:#1240A0">Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup</a></b></font><br>
        <i> Siyuan Li*, Zicheng Liu*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Di Wu, Zihan Liu, Stan Z. Li‚Ä† </a></i><br><i><b>arXiv, 2022</b></i><br>
        [<a href= "https://arxiv.org/abs/2111.15454" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/arXiv_2021_SAMix_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>


</ul>
<br />


<p><b>Conferences (As First Author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_VQDNA.jpeg" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#1240A0">VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Zicheng Liu, Di Wu, Cheng Tan, Jiangbin Zheng, Yufei Huang, Stan Z. Li‚Ä† </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Lupin1998/VQDNA" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/icml_2024_VQDNA_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_MogaNet.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2211.03295" target="_blank" style="color:#1240A0">MogaNet: Multi-order Gated Aggregation Network</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Li‚Ä† </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2211.03295" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#1240A0">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/582542948" target="_blank" style="color:#1240A0">Zhihu</a>]
        [<a href="./Files/iclr_2024_MogaNet_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>

</ul>
<br />


<p><b>Conferences (As Co-author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_CHELA.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#1240A0">Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences</a></b></font><br>
        <i> Zicheng Liu, Siyuan Li, Li Wang, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Yunfan Liu, Stan Z. Li‚Ä† </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/icml_2024_CHELA_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/IJCAI24_LongVQ.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#1240A0">LongVQ: Long Sequence Modeling with Vector Quantization on Structured Memory</a></b></font><br>
        <i> Zicheng Liu, Li Wang, Siyuan Li, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Haitao Lin, Stan Z. Li‚Ä† </a></i><br><i><b><a href="https://ijcai24.org" target="_blank" style="color:#000000">International Joint Conference on Artificial Intelligence (IJCAI), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/ijcai_2024_LongVQ_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>
    

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_SemiReward.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2310.03013" target="_blank" style="color:#1240A0">SemiReward: A General Reward Model for Semi-supervised Learning</a></b></font><br>
        <i> Siyuan Li*, Weiyang Jin*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Li‚Ä† </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2310.03013" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/SemiReward" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/iclr_2024_SemiReward_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/NeurIPS23_OpenSTL.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2306.11249" target="_blank" style="color:#1240A0">OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning</a></b></font><br>
        <i> Cheng Tan*, Siyuan Li*, Zhangyang Gao, Wenfei Guan, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Zicheng Liu, Lirong Wu, Stan Z. Li‚Ä† </a></i><br><i><b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">Advances in Neural Information Processing Systems (NeurIPS), 2023</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2306.11249" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#1240A0">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/640271275" target="_blank" style="color:#1240A0">Zhihu</a>]
        [<a href="./Files/NIPS_2023_OpenSTL_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>   


</ul>
<br />

<!--
<p><b>Journals</b>: </p>
-->
<font size="3"> 
<ul>

</ul>
<br />


<A NAME="Services"><h2>Professional Services</h2></A>

<p><b>Program Committee Member / Reviewer</b>: </p>
<font size="3"> 
<ul>
<!--
<li><b>Conference Reviewer / PC Member:</b></li>-->
<li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b><a href="https://cvpr.thecvf.com" target="_blank" style="color:#000000">CVPR</a></b>), 2025<br /></li>
<li>International Conference on Learning Representations (<b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR</a></b>), 2024 (TinyPapers), 2025<br /></li>
<li>International Conference on Machine Learning (<b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML</a></b>), 2024<br /></li>
<li>European Conference on Computer Vision (<b><a href="https://eccv.ecva.net" target="_blank" style="color:#000000">ECCV</a></b>), 2024<br /></li>
<li>Conference on Neural Information Processing Systems (<b><a href="https://neurips.cc" target="_blank" style="color:#000000">NeurIPS</a></b>), 2024 (D&B Track)<br /></li>
<li>AAAI Conference on Artificial Intelligence (<b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI</a></b>), 2025<br /></li>
<li>ACM International Conference on Multimedia (<b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM</a></b>), 2024<br /></li>
<li>BMVA The British Machine Vision Conference (<b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC</a></b>), 2024<br /></li>
<li>IAPR International Conference on Pattern Recognition (<b><a href="https://icpr2024.org" target="_blank" style="color:#000000">ICPR</a></b>), 2024<br /></li>
<!--
<li><b>Journal Reviewer:</b></li>
-->
</ul>
</font>
<br />
 
<p><b>Membership</b>: </p>
<font size="3"> 
<ul>
<li>China Computer Federation (<b><a href="https://www.ccf.org.cn/en/" target="_blank" style="color:#000000">CCF</a></b>), Student Member, 2024-2026</li>
<li>China Society of Image and Graphics (<b><a href="https://en.csig.org.cn" target="_blank" style="color:#000000">CSIG</a></b>), Student Member, 2023</li>
</ul>
</font>
<br />
<br />


<A NAME="Awards"><h2>Selected Awards and Honors</h2></A>
<b>2024:</b>
<ul>
    <li><a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#000000">Outstanding Reviewer (<font color="#FF0000">rate: 2.7%, 198/7293</font>)</a>, awarded by The 18th European Conference on Computer Vision (<b><a href="https://eccv.ecva.net/Conferences/2024" target="_blank" style="color:#000000">ECCV 2024</a></b>). </li>
    <li><a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#000000">Outstanding Reviewer (<font color="#FF0000">rate: 139/X</font>)</a>, awarded by ACM International Conference on Multimedia 2024 (<b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM 2024</a></b>). </li>
    <li><a href="https://bmvc2024.org/people/reviewers/" target="_blank" style="color:#000000">Outstanding Reviewer (<font color="#FF0000">rate: 19.3%, 166/860</font>)</a>, awarded by The 35th British Machine Vision Conference (<b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC 2024</a></b>). </li>
</ul>

<br />

<img src="./Awards/ECCV24_Outstadning_Reviewer.png" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="320px" />&nbsp;</td>
<br />

<!-- 
<p><b>Invited Talk</b>: </p>
<font size="3"> 
<ul>
<li>2023/12/14: Talk on "Mixup Data Augmentation for Computer Vision" @ Chongqing Technology and Business University</li> [<a href="./Files/Mixup_Data_Augmentation_for_Computer_Vision_20231214.pdf" target="_blank" style="color:#2a7ce0">PPT</a>]
<li>2023/12/14: Talk on "Introduction to AI Research and Experience Sharing" @ Chongqing Technology and Business University</li>
</ul>
</font>
<br />
-->

<A NAME="Acknowledgement"><h2>Acknowledgement</h2></A>
My research career cannot be possible without the generous support from all my awesome mentors, collaborators, and friends:
<ul>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Xinggang Wang</a>, Prof. <a href="https://yuzhou.vlrlab.net" target="_blank" style="color:#1240A0">Yu Zhou</a>, Prof. Xin Yang at Huazhong University of Science and Technology (HUST).</li>
<li>Chair Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Stan Z. Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#1240A0">Siyuan Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#1240A0">Zicheng Liu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=o5A23qIAAAAJ" target="_blank" style="color:#1240A0">Haitao Lin</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=egz8bGQAAAAJ" target="_blank" style="color:#1240A0">Jiangbin Zheng</a>, Mr. Siqi Ma at Westlake University.</li>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Dan Xu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=ennCRJAAAAAJ" target="_blank" style="color:#1240A0">Zhenxing Mi</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=4OuUHYQAAAAJ" target="_blank" style="color:#1240A0">Yuxin Wang</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=kR5LuzgAAAAJ" target="_blank" style="color:#1240A0">Yu Cai</a>, Mr. <a href="https://scholar.google.com/citations?hl=en&user=1xsk9rYAAAAJ" target="_blank" style="color:#1240A0">Yiwei Chen</a> at The Hong Kong University of Science & Technology.</li>
<li>Dr. <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#1240A0">Bin Fu</a>, Mr. <a href="https://scholar.google.com/citations?hl=en&user=Ww37Oi4AAAAJ" target="_blank" style="color:#1240A0">Aozhong Zhang</a> at SIAT-MMLab, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences.</li>
<li>Dr. <a href="https://scholar.google.com/citations?hl=en&user=TpX2C3cAAAAJ" target="_blank" style="color:#1240A0">Xiaoping Du</a> at Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences.</li>
<li>Mr. <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#1240A0">Juanxi Tian</a> at Hong Kong Baptist University.</li>
</ul>
<div style="text-align:justify">
Apart from AI research, I've been fortunate to meet wonderful friends and partners at different life stages. Whether we are still in touch or not, I appreciate all your presence and support and wish you all the best. To the very best of times!
</div>
<br />



<A NAME="Fragments of Memories"><h2>Fragments of Memories</h2></A>
Great memories with my advisors: <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a> (HUST), <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Chair Prof. Stan Z. Li</a> (Westlake University), and also CAIRI AI Lab.
</p>
<img src="./Files/Zedong_With_Prof_Xinggang.jpeg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="190px" />&nbsp;</td>
<img src="./Files/Zedong_With_Prof_Stan.jpeg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="340px" />&nbsp;</td>
<img src="./Files/CAIRI_WODeco.jpg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="382px" />&nbsp;</td>
<!-- 
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/889375a9-27d8-4975-bfa1-05b36916e021" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="352px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/d4a10ac1-2dbc-4e41-b286-4c1ffbd2954d" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="349px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/e55bbe34-3513-4cf0-8f6b-ef4c93f155ff" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="428px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/bd91d46c-0c14-484d-ac46-650f2c682e23" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="493px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/a414637c-a728-4695-b26c-2b00053d392a" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="460px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/0b34630b-5231-4742-8896-7e5851445cca" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="460px" />&nbsp;</td>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=n&d=p1SE8mvpEDXWAl_SKZ3NCPEvAjvDm5pAjUQB-nDQa5U&co=203b62&cmo=ebb653&cmn=ff5353&ct=ffffff' async="async"></script>
-->

<A NAME=""><h2></h2></A>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=305&t=n&d=p1SE8mvpEDXWAl_SKZ3NCPEvAjvDm5pAjUQB-nDQa5U&cmo=edbf4a&cmn=ff5f5f&co=306285'></script>

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {  
            $("#back_top").fadeIn(400); 
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script>




<!-- <font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2022.06.01</p>
</font> -->
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;All Rights Reserved by Zedong Wang. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.

</body>
</html>
