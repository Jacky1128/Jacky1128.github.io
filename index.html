<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Zedong Wang, Deep learning, Computer Vision, HKUST">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/HKUST_logo.png">
<title>Zedong Wang</title>
<script async defer src="https://buttons.github.io/buttons.js"></script>

</head>


<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./" style="color:#002FA7"><img src="./Files/Profile_Twitter.jpeg" alt="" height="222px" /></a>&nbsp;</td>
<td align="left"><p><a href="./" style="color:#000000"><font size="5">WANG, Ze Dong çŽ‹æ³½æ ‹</font></a><br />
<i> <a href="https://cse.hkust.edu.hk/admin/factsheet/" target="_blank" style="color:#002FA7">Department of Computer Science and Engineering (CSE)</a> </i>
<br />
<i> <a href="https://hkust.edu.hk" target="_blank" style="color:#002FA7">The Hong Kong University of Science and Technology (HKUST)</a> </i>

<br /><br />
Location: RPg Hub 1007, Academic Building, HKUST, Clear Water Bay, Kowloon, Hong Kong (<a href="https://www.google.com/maps/search/HKUST+map/@22.335677,114.2613186,17z" target="_blank" style="color:#002FA7">MAP</a>).<br />


<class="staffshortcut">
 <A HREF="#News" style="color:#002FA7">News</A> | 
 <A HREF="#Interests" style="color:#002FA7">Interests</A> | 
 <A HREF="#Education" style="color:#002FA7">Education</A> | 
 <A HREF="#Internship" style="color:#002FA7">Intern</A> | 
 <A HREF="#Publications" style="color:#002FA7">Publications</A> | 
 <A HREF="#Services" style="color:#002FA7">Services</A> | 
 <A HREF="#Awards" style="color:#002FA7">Awards</A> | 
 <A HREF="#Acknowledgement" style="color:#002FA7">Notes</A> | 
 <A HREF="./Files/Jacky_CV.pdf" style="color:#002FA7">ResumÃ© (CV)</A> 
<br />
<br />

Email: zedong.wang@connect.ust.hk (preferred); zwangmw@cse.ust.hk; wangzedong@westlake.edu.cn;<br />
[<a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#002FA7">Google Scholar</a>]  
[<a href="https://openreview.net/profile?id=~Zedong_Wang1" target="_blank" style="color:#002FA7">OpenReview</a>]
[<a href="https://twitter.com/ZedongWangAI" target="_blank" style="color:#002FA7">Twitter (X)</a>]
[<a href="https://huggingface.co/ZedongWangAI" target="_blank" style="color:#002FA7">Hugging Face</a>] 
[<a href="https://github.com/Jacky1128" target="_blank" style="color:#002FA7">GitHub</a>] 
[<a href="https://orcid.org/0009-0000-0112-0491" target="_blank" style="color:#002FA7">ORCID</a>]  
[<a href="https://www.threads.net/@jackyw_28?hl=en" target="_blank" style="color:#002FA7">Threads</a>] 
[<a href="https://www.strava.com/athletes/104629081" target="_blank" style="color:#002FA7">Strava</a>]


<br />
<br />
<i>Welcome to contact me about research or internship on related topics. Feel free to drop me emails. </i><br />
</td></tr></table>



<A NAME="Short Bio"><h2>Short Bio</h2></A>
<div style="text-align:justify; line-height:132%">
Zedong Wang (Jacky) is currently a first-year <a href="https://cse.hkust.edu.hk/admin/factsheet/" target="_blank" style="color:#002FA7">CSE</a> PhD student at The Hong Kong University of Science and Technology (<a href="https://hkust.edu.hk" target="_blank" style="color:#002FA7">HKUST</a>), 
advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>. Previously, he was a visiting student at <a href="https://en.westlake.edu.cn" target="_blank" style="color:#002FA7">Westlake University</a>. He received his bachelor's degree in Electronic & Information Engineering from Huazhong University of Science and Technology (<a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#002FA7">HUST</a>) in 2023, where he was fortunately advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.
</p>
His research centers on computer vision, multi-task and multi-modal learning, with a specific focus on efficient/fresh network architectures, unified learning paradigms, and optimization techniques. He has been recognized as Outstanding/Top Reviewer at prestigious conferences, such as <a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#002FA7">ECCV'24</a>, <a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#002FA7">MM'24</a>, and <a href="http://bmvc2024.org/people/reviewers/" target="_blank" style="color:#002FA7">BMVC'24</a>.





<A NAME="News"><h2>News</h2></A>
  <!-- <div style="height:200px;overflow-y:auto;background:#ffffff;"> -->
  <div style="height:250px;overflow-y:auto;">
  <ul>
    <li><b> <font color="#9F6821">[2025.04.04]</font> </b> Co-authored paper on <i>unified visual generation&representation</i>, <i><a href="https://huggingface.co/papers/2504.00999" target="_blank"> MergeVQ</a></i>, is accepted at <b><a href="https://cvpr.thecvf.com/Conferences/2025" target="_blank" style="color:#000000">CVPR 2025</a></b>, <a href="https://huggingface.co/papers/date/2025-04-03" target="_blank" style="color:#002FA7"><b>ranking 1st</b> in ðŸ¤—Hugging Face Daily Papers,</a> <a href="https://huggingface.co/papers/week/2025-W14" target="_blank" style="color:#002FA7">and <b>ranking 4th</b> in Weekly Papers</a>. Congrats to all co-authors! </li></li>
    <li><b> <font color="#9F6821">[2024.12.12]</font> </b> Co-authored paper on <i>low-level vision</i>, <i><a href="https://arxiv.org/abs/2503.01136" target="_blank"> PGHHNet</a></i>, is accepted at <b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI 2025</a></b>. Congrats to all co-authors!</li></li>
    <li><b> <font color="#9F6821">[2024.11.04]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> at <b><a href="https://bmvc2024.org/people/reviewers/" target="_blank" style="color:#000000">BMVC 2024 <font color="#b80000">(rate: 19.3%, 166/860)</font></a></b>. Happy to contribute to the community! Huge thanks to the BMVC organizers for their dedication.</li></li>
    <li><b> <font color="#9F6821">[2024.11.04]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> at <b><a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#000000">ACM MM 2024 <font color="#b80000">(rate: 139/X)</font></a></b>. Happy to contribute to the community! Huge thanks to the ACM MM organizers for their dedication.</li></li>
    <li><b> <font color="#9F6821">[2024.10.10]</font> </b> One paper on <i>vision backbones & optimizers</i>, <i><a href="https://arxiv.org/abs/2410.06373" target="_blank">BOCB</a></i>, is released. Welcome to visit <a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">Hugging Face</a> and <a href="https://bocb-ai.github.io" target="_blank" style="color:#002FA7">Project</a> page.</li></li>
    <li><b> <font color="#9F6821">[2024.09.24]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> at <b><a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#000000">ECCV 2024 <font color="#b80000">(rate: 2.7%, 198/7293)</font></a></b>. Happy to contribute to the community! Huge thanks to the ECCV organizers for their dedication.</li></li>
    <li><b> <font color="#9F6821">[2024.07.14]</font> </b> Maintain an open-source repository on <i>optimizers</i>, <i><a href="https://github.com/tianshijing/Awesome-Optimizers" target="_blank" >Awesome-Optimizers</a></i>, with <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#002FA7">Juanxi Tian</a> and <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>. </li>
    <li><b> <font color="#9F6821">[2024.05.02]</font> </b> One paper on <i>AI4Science (genomics)</i>, <i><a href="https://arxiv.org/abs/2405.10812" target="_blank" > VQDNA</a></i>, is accepted at <b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML 2024</a></b>. </li></li>
    <li><b> <font color="#9F6821">[2024.05.02]</font> </b> One co-authored paper on <i>long sequence modeling</i>, <i><a href="https://arxiv.org/abs/2406.08128" target="_blank" > CHELA</a></i>, is accepted at <b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#002FA7">Zicheng Liu</a>! </li></li>
    <li><b> <font color="#9F6821">[2024.04.16]</font> </b> One co-authored paper on <i>long sequence modeling</i>, <i><a href="https://arxiv.org/abs/2404.11163" target="_blank" > LongVQ</a></i>, is accepted at <b><a href="https://ijcai24.org" target="_blank" style="color:#000000">IJCAI 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#002FA7">Zicheng Liu</a>! </li></li>
    <li><b> <font color="#9F6821">[2024.01.16]</font> </b> One paper on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" > MogaNet</a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code & weights</a> (180 stars) are released!</li></li>
    <li><b> <font color="#9F6821">[2024.01.16]</font> </b> One co-authored paper on <i>semi-sup learning</i>, <i><a href="https://arxiv.org/abs/2310.03013" target="_blank" > SemiReward</a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>! </li></li>
    <li><b> <font color="#9F6821">[2024.01.09]</font> </b> I am invited to serve as an <i> emergency reviewer</i> at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b> <b><a href="https://iclr.cc/Conferences/2024/CallForTinyPapers" target="_blank" style="color:#000000">(TinyPapers)</a></b>. It will be held in Vienna, Austria. </li></li>
    <li><b> <font color="#9F6821">[2023.12.31]</font> </b> One co-authored preprint on <i>self-supervised learning</i>, <i><a href="https://arxiv.org/abs/2401.00897" target="_blank" > Masked Modeling on Vision and Beyond</a></i>. </li></li>
    <li><b> <font color="#9F6821">[2023.09.31]</font> </b> One co-authored paper on <i>video prediction</i>, <i><a href="https://arxiv.org/abs/2306.11249" target="_blank" > OpenSTL</a></i>, is accepted at <b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">NeurIPS 2023</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=6kTV6aMAAAAJ" target="_blank" style="color:#002FA7">Cheng Tan</a>! </li></li>
    <li><b> <font color="#9F6821">[2023.06.25]</font> </b> Got my B.Eng. degree from Huazhong University of Science and Technology (HUST)! Special thanks to my undergraduate supervisor <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Prof. Xinggang Wang</a> for the generous support!</li>
    <li><b> <font color="#9F6821">[2023.05.23]</font> </b> One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2111.15454" target="_blank" > SAMix</a></i>, is presented for both SL & SSL scenarios. </li>
    <li><b> <font color="#9F6821">[2022.11.07]</font> </b> One preprint on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" > MogaNet</a></i>. A new family of pure convolutional architecture covering <b>5M~100M+</b> model scales with great performance.  <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code & weights</a> are released (180 stars). Welcome to discuss, use, and star!</li>
    <li><b> <font color="#9F6821">[2022.09.11]</font> </b> One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2209.04851" target="_blank" >OpenMixup</a></i>, is presented for vision tasks. This is also my first arXiv paper!</li>
    <li><b> <font color="#9F6821">[2022.09.11]</font> </b> Maintain an open-source repository, <i><a href="https://github.com/Westlake-AI/openmixup" target="_blank" >OpenMixup</a></i> (618 stars), for both supervised, semi- and self-supervised visual representation learning based on PyTorch. On updating! </li>
    <li><b> <font color="#9F6821">[2022.07.06]</font> </b> Fortunate to become <i>visiting student</i> at Westlake University, under the supervision of <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Chair Prof. Stan Z. Li</a>. </li></li>
    <li><b> <font color="#9F6821">[2021.09.01]</font> </b> Fortunate to become <i>research intern</i> in <a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#002FA7"> HUST Vision Lab</a>, under the supervision of <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Prof. Xinggang Wang</a>. </li></li>
    <li><b> <font color="#9F6821">[2021.06.01]</font> </b> Fortunate to become <i>research intern</i> in <a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#002FA7"> SIAT-MMLab</a> at Shenzhen Institute of Advanced Technology, CAS. </li></li>

</ul>
</div>


<A NAME="Interests"><h2>Research Interests</h2></A>
Currently, I focus mostly on Computer Vision, efficient Multi-task and Multi-modal Learning, including (but not limited to):
<ul>
    <li>Data-efficient Learning: Mixup Augmentation [<a href="https://arxiv.org/abs/2209.04851" target="_blank" style="color:#002FA7">OpenMixup</a>, <a href="https://arxiv.org/abs/2111.15454" target="_blank" style="color:#002FA7">SAMix</a>], Semi-supervised Learning [<a href="https://arxiv.org/abs/2310.03013" target="_blank" style="color:#002FA7">SemiReward</a>].</li>
    <li>Efficient Network Architectures: Vision Backbones [<a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#002FA7">MogaNet</a>], Multi-task Architectures [<a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#002FA7">MergeVQ</a>].</li>
    <li>Optimization Techniques: Optimizers [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#002FA7">SEMA</a>, <a target="_blank" style="color:#002FA7">BOCB</a>], Multi-task Optimization.</li>
</ul>





<A NAME="Education"><h2>Education</h2></A>

<p style="line-height: 0.2;">&nbsp; </p>

<p style="margin-left: 1.9em;"> 
    <img src="Files/HKUST_logo.png" align="left" width="55" height="78"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://hkust.edu.hk" target="_blank" style="color:#000000">The Hong Kong University of Science and Technology</a> (2025-now) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; Ph.D. in Computer Science and Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Multi-task and Multi-modal Learning. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Supervisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>.<br>
</p>
<p style="line-height: 0.4;">&nbsp; </p>

<p style="margin-left: 1.0em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="67"/>
    &nbsp&nbsp&nbsp&nbsp <b><a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#000000">Huazhong University of Science and Technology</a> (2019-2023) </b><br> 
    &nbsp&nbsp&nbsp&nbsp &bull; B.Eng. in Electronic and Information Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Thesis:</i> Efficient ConvNet-based Vision Backbone for Multiple Tasks (Grade: 92/100). <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Supervisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.<br>
</p>
<p style="line-height: 0.1;">&nbsp; </p>

<A NAME="Internship"><h2>Research Experience</h2></A>

<p style="line-height: 0.3;">&nbsp; </p>

<p style="margin-left: 1.7em;"> 
    <img src="Files/ZEEKR_logo.png" align="left" width="65" height="65"/>
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://en.wikipedia.org/wiki/Zeekr" target="_blank" style="color:#000000">ZEEKR Intelligent Technology</a> (2024.04-2025.01) </b><br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Multi-task Optimization. <br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a> (University-Enterprise Cooperation). <br>
</p>

<p style="line-height: 0.6;">&nbsp; </p>


<p style="margin-left: 1.9em;"> 
    <img src="Files/westlake.ico" align="left" width="62" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#000000">Stan Z. Li's AI Lab, Westlake University</a> (2022.06-2024.03) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Visual Representation Learning and AI for Science. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> Chair Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Stan Z. Li</a> (IEEE Fellow, IAPR Fellow).<br>
</p>

<p style="line-height: 0.6;">&nbsp; </p>

<p style="margin-left: 1.2em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="65"/>
    &nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#000000">Xinggang Wang's Vision Group, HUST</a> (2021.09-2022.06) </b><br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Few-shot Semantic Segmentation. <br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.<br>
</p>

<p style="line-height: 0.6;">&nbsp; </p>

<p style="margin-left: 1.9em;"> 
    <img src="Files/SIAT_logo.png" align="left" width="64" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; <b><a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#000000">SIAT-MMLab, Chinese Academy of Sciences</a> (2021.06-2021.09) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Topics:</i> Semantic Segmentation and Text Spotting. <br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Advisor:</i> Dr. <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#002FA7">Bin Fu</a>.<br>
</p>

<p style="line-height: 0.3;">&nbsp; </p>

<A NAME="Publications"><h2>Publications</h2></A>

<p><b>Selected Preprints (*: Equal Contribution. â€ : Corresponding Author.)</b>: </p>
<font size="3"> 
<ul>
<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_BOCB.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a target="_blank" style="color:#b80000">[NEW!]</a></b> <b><a href= "https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning</a></b></font><br>
        <i> Siyuan Li*, Juanxi Tian*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u>*</a></b>, Luyuan Zhang, Zicheng Liu, Weiyang Jin, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2410.06373" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">ðŸ¤—HF</a>]
        [<a href="https://bocb-ai.github.io" target="_blank" style="color:#002FA7">Project</a>]
        [<a href="https://github.com/Black-Box-Optimization-Coupling-Bias/BOCB" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_SEMA.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://huggingface.co/papers/2402.09240" target="_blank" style="color:#002FA7">Switch EMA: A Free Lunch for Better Flatness and Sharpness</a></b></font><br>
        <i> Siyuan Li*, Zicheng Liu*, Juanxi Tian*, Ge Wang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2402.09240" target="_blank" style="color:#002FA7">ðŸ¤—HF</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2023_MM_Survey.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#002FA7">Masked Modeling for Self-supervised Representation Learning on Vision and Beyond</a></b></font><br>
        <i> Siyuan Li*, Luyuan Zhang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Di Wu, Lirong Wu, Zicheng Liu, Jun Xia, Cheng Tan, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2023</b></i><br>
        [<a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Lupin1998/Awesome-MIM" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2023_MIMSurvey_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>



</ul>
<br />


<p><b>Conferences (As First Author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_VQDNA.jpeg" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://proceedings.mlr.press/v235/li24bm.html" target="_blank" style="color:#002FA7">VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u>*</a></b>, Zicheng Liu, Di Wu, Cheng Tan, Jiangbin Zheng, Yufei Huang, Stan Z. Liâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Lupin1998/VQDNA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/icml_2024_VQDNA_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_MogaNet.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://iclr.cc/virtual/2024/poster/18447" target="_blank" style="color:#002FA7">MogaNet: Multi-order Gated Aggregation Network</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u>*</a></b>, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Liâ€  </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2211.03295" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/582542948" target="_blank" style="color:#002FA7">Zhihu</a>]
        [<a href="./Files/iclr_2024_MogaNet_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

</ul>

<p><b>Conferences (As Co-author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Teasers/CVPR25_MergeVQ.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a target="_blank" style="color:#b80000">[NEW!]</a></b><b><a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#002FA7">MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization</a></b></font><br>
        <i>Siyuan Li*, Luyuan Zhang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Juanxi Tian, Cheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie, Haonan Lu, Haoqian Wang, Zhen Leiâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">IEEE/CVF Conference on Computer Vision (CVPR), 2025</a></b></i><br>
        [<a href="https://arxiv.org/abs/2504.00999" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#002FA7">ðŸ¤—HF</a>]
        [<a href="https://apexgen-x.github.io/MergeVQ/" target="_blank" style="color:#002FA7">Project</a>]
        [<a href="./Files/cvpr_2025_MergeVQ_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_CHELA.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://proceedings.mlr.press/v235/liu24ak.html" target="_blank" style="color:#002FA7">Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences</a></b></font><br>
        <i> Zicheng Liu, Siyuan Li, Li Wang, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Yunfan Liu, Stan Z. Liâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/icml_2024_CHELA_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/IJCAI24_LongVQ.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://www.ijcai.org/proceedings/2024/510" target="_blank" style="color:#002FA7">LongVQ: Long Sequence Modeling with Vector Quantization on Structured Memory</a></b></font><br>
        <i> Zicheng Liu, Li Wang, Siyuan Li, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Haitao Lin, Stan Z. Liâ€  </a></i><br><i><b><a href="https://ijcai24.org" target="_blank" style="color:#000000">International Joint Conference on Artificial Intelligence (IJCAI), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/ijcai_2024_LongVQ_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>
    

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_SemiReward.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://iclr.cc/virtual/2024/poster/18245" target="_blank" style="color:#002FA7">SemiReward: A General Reward Model for Semi-supervised Learning</a></b></font><br>
        <i> Siyuan Li*, Weiyang Jin*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Liâ€  </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2310.03013" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/SemiReward" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/iclr_2024_SemiReward_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/NeurIPS23_OpenSTL.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://papers.nips.cc/paper_files/paper/2023/hash/dcbff44d11130e75d09d3930411c23e1-Abstract-Datasets_and_Benchmarks.html" target="_blank" style="color:#002FA7">OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning</a></b></font><br>
        <i> Cheng Tan*, Siyuan Li*, Zhangyang Gao, Wenfei Guan, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Zicheng Liu, Lirong Wu, Stan Z. Liâ€  </a></i><br><i><b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">Advances in Neural Information Processing Systems (NeurIPS), 2023</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2306.11249" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#002FA7">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/640271275" target="_blank" style="color:#002FA7">Zhihu</a>]
        [<a href="./Files/NIPS_2023_OpenSTL_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>   


</ul>


<A NAME="Services"><h2>Professional Services</h2></A>

<p><b>Conference Reviewer/PC Member:</b> </p>
<font size="3"> 
<ul>

<li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b><a href="https://cvpr.thecvf.com" target="_blank" style="color:#000000">CVPR</a></b>), 2025<br /></li>
<li>IEEE/CVF International Conference on Computer Vision (<b><a href="https://iccv.thecvf.com" target="_blank" style="color:#000000">ICCV</a></b>), 2025<br /></li>
<li>European Conference on Computer Vision (<b><a href="https://eccv.ecva.net" target="_blank" style="color:#000000">ECCV</a></b>), 2024<br /></li>
<li>International Conference on Learning Representations (<b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR</a></b>), 2024 (TinyPapers), 2025<br /></li>
<li>International Conference on Machine Learning (<b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML</a></b>), 2024, 2025<br /></li>
<li>Conference on Neural Information Processing Systems (<b><a href="https://neurips.cc" target="_blank" style="color:#000000">NeurIPS</a></b>), 2024 (D&B Track), 2025<br /></li>
<li>AAAI Conference on Artificial Intelligence (<b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI</a></b>), 2025<br /></li>
<li>ACM International Conference on Multimedia (<b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM</a></b>), 2024<br /></li>
<li>BMVA The British Machine Vision Conference (<b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC</a></b>), 2024<br /></li>
<li>IAPR International Conference on Pattern Recognition (<b><a href="https://icpr2024.org" target="_blank" style="color:#000000">ICPR</a></b>), 2024<br /></li>
</ul>
</font>
<br />
 
<p><b>Journal Reviewer:</b> </p>
<font size="3"> 
<ul>
<li>IEEE Transactions on Knowledge and Data Engineering (<b><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69" target="_blank" style="color:#000000">TKDE</a></b>)</li>
</ul>
</font>
<br />

<p><b>Membership</b>: </p>
<font size="3"> 
<ul>
<li>China Computer Federation (<b><a href="https://www.ccf.org.cn/en/" target="_blank" style="color:#000000">CCF</a></b>), Student Member, 2024-2026</li>
<li>China Society of Image and Graphics (<b><a href="https://en.csig.org.cn" target="_blank" style="color:#000000">CSIG</a></b>), Student Member, 2023</li>
</ul>
</font>
<br />


<A NAME="Awards"><h2>Selected Awards and Honors</h2></A>
<p style="line-height: 0.1;">&nbsp; </p>

<ul>
    <li><b>Outstanding/Top Reviewer Award,</b> at <a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#002FA7">ECCV 2024</a> <b><font color="#b80000">(2.7%, 198/7293)</font></b>, <a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#002FA7">ACM MM 2024</a> <b><font color="#b80000">(139/X)</font></b>, and <a href="https://bmvc2024.org/people/reviewers/" target="_blank" style="color:#002FA7">BMVC 2024</a> <b><font color="#b80000">(19.3%, 166/860)</font></b>. </li>
    <li><b>Westlake University Summer Studentship (2/100+),</b> selected by Stan Z. Li's AI Lab, 2022. </li>
</ul>


<A NAME="Acknowledgement"><h2>Acknowledgement</h2></A>
My research career cannot be possible without the support from all my awesome mentors, collaborators, and friends:
<ul>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>, Prof. <a href="https://yuzhou.vlrlab.net" target="_blank" style="color:#002FA7">Yu Zhou</a> at HUST.</li>
<li>Chair Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Stan Z. Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#002FA7">Zicheng Liu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=o5A23qIAAAAJ" target="_blank" style="color:#002FA7">Haitao Lin</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=egz8bGQAAAAJ" target="_blank" style="color:#002FA7">Jiangbin Zheng</a>, Mr. Siqi Ma at Westlake University.</li>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=ennCRJAAAAAJ" target="_blank" style="color:#002FA7">Zhenxing Mi</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=4OuUHYQAAAAJ" target="_blank" style="color:#002FA7">Yuxin Wang</a>, Mr. <a href="https://wangandyyucheng.github.io" target="_blank" style="color:#002FA7">Yucheng Wang</a>, Mr. <a href="https://scholar.google.com/citations?hl=en&user=1xsk9rYAAAAJ" target="_blank" style="color:#002FA7">Yiwei Chen</a> at HKUST.</li>
<li>Mr. <a href="https://scholar.google.com/citations?hl=en&user=Ww37Oi4AAAAJ" target="_blank" style="color:#002FA7">Aozhong Zhang</a> at SUNY Albany.</li>
<li>Mr. <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#002FA7">Juanxi Tian</a> at HKBU.</li>
</ul>
<div style="text-align:justify">
<p style="line-height: 0.1;">&nbsp; </p>
Beyond academia, I feel incredibly fortunate to met wonderful friends and partners along the way (particularly during my middle school and high school years in Shenzhen). To every one of you who has walked through different chapters of my life: I appreciate all your presence and support, and carry deep gratitude for our shared moments. Whether we are still in touch or not, I wish you all the best!
<p style="line-height: 0.1;">&nbsp; </p>
Even if most of you may never visit this page, please know my door is always open for a chat - coffee's on me!
</div>
<br />



<A NAME="Fragments of Memories"><h2>Fragments of Memories</h2></A>
Great memories with my advisors: Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a> (HUST). 
</p>

<img src="./Files/Zedong_With_Prof_Xinggang.jpeg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="190px" />&nbsp;</td>


<br />

<A NAME=""><h2></h2></A>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=305&t=n&d=p1SE8mvpEDXWAl_SKZ3NCPEvAjvDm5pAjUQB-nDQa5U&cmo=edbf4a&cmn=ff5f5f&co=306285'></script>

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {  
            $("#back_top").fadeIn(400); 
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script>




<!-- <font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2022.06.01</p>
</font> -->
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;All Rights Reserved by Zedong Wang. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.

</body>
</html>
