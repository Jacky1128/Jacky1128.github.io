<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Zedong Wang, Deep learning, Computer Vision, HKUST">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/HKUST_logo.png">
<title>Zedong Wang</title>
<script async defer src="https://buttons.github.io/buttons.js"></script>

</head>


<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./" style="color:#002FA7"><img src="./Files/Profile_Home.jpg" alt="" height="208px" /></a>&nbsp;</td>
<td align="left"><p><a href="./" style="color:#000000"><font size="5">WANG, Ze Dong çŽ‹æ³½æ ‹</font></a><br />
<i> <a href="https://cse.hkust.edu.hk/admin/factsheet/" target="_blank" style="color:#002FA7">Department of Computer Science and Engineering (CSE)</a> </i>
<br />
<i> <a href="https://hkust.edu.hk" target="_blank" style="color:#002FA7">The Hong Kong University of Science and Technology (HKUST)</a> </i>

<br />
<div style="display:flex; flex-direction:column; gap:3px;">
    <div>Address: RPg Hub 1007, Academic Building, HKUST, Clear Water Bay, Kowloon, Hong Kong (<a href="https://www.google.com/maps/search/HKUST+map/@22.335677,114.2613186,17z" target="_blank" style="color:#002FA7">MAP</a>).</div>
    <div class="navigation-bar">
        <a href="#News" class="nav-link">News</a> | 
        <a href="#Interests" class="nav-link">Interests</a> | 
        <a href="#Education" class="nav-link">Education</a> | 
        <a href="#Internship" class="nav-link">Interns</a> | 
        <a href="#Publications" class="nav-link">Pubs</a> | 
        <a href="#Services" class="nav-link">Services</a> | 
        <a href="#Awards" class="nav-link">Awards</a> | 
        <a href="#Acknowledgement" class="nav-link">Acknowledgement</a> | 
        <a href="./Files/Jacky_CV.pdf" class="nav-link">ResumÃ© (CV)</a>
  </div></div>
  
<style>
  .navigation-bar {
    gap: 4px;
    margin: 0 0 15px 0; /* Removed top margin */
  }
  .nav-link {
    color: #002FA7;
    text-decoration: none;
    border-radius: 5px;
  }
  .nav-link:hover {
    background-color: #e6eeff;
  }
</style>

<!-- Add in the head section -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

Email: zedong.wang@connect.ust.hk (preferred); zwangmw@cse.ust.hk; wangzedong@westlake.edu.cn;<br />
[<a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#002FA7">Google Scholar</a>]  
[<a href="https://openreview.net/profile?id=~Zedong_Wang1" target="_blank" style="color:#002FA7">OpenReview</a>]
[<a href="https://twitter.com/ZedongWangAI" target="_blank" style="color:#002FA7">Twitter (X)</a>]
[<a href="https://huggingface.co/ZedongWangAI" target="_blank" style="color:#002FA7">Hugging Face</a>] 
[<a href="https://github.com/Jacky1128" target="_blank" style="color:#002FA7">GitHub</a>] 
[<a href="https://orcid.org/0009-0000-0112-0491" target="_blank" style="color:#002FA7">ORCID</a>]  
[<a href="https://www.threads.com/@jackyw_28" target="_blank" style="color:#002FA7">Threads</a>]  
[<a href="https://www.strava.com/athletes/104629081" target="_blank" style="color:#002FA7">Strava</a>]


<br />
<br />
<i><a href="https://x.com/fchollet/status/1894227128302145970" target="_blank" style="color:#000000">Humor, art, science, and being kind to each other is how you preserve your sanity in a darkening world.</a>
</i><br />
</td></tr></table>


<A NAME="Short Bio"><h2>Short Bio</h2></A>
<div style="text-align:justify; line-height:132%">
Zedong Wang (Jacky) is currently a first-year <a href="https://cse.hkust.edu.hk/admin/factsheet/" target="_blank" style="color:#002FA7">CSE</a> PhD student at The Hong Kong University of Science and Technology (<a href="https://hkust.edu.hk" target="_blank" style="color:#002FA7">HKUST</a>), 
advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>. Previously, he was a visiting student at <a href="https://en.westlake.edu.cn" target="_blank" style="color:#002FA7">Westlake University</a>. He received his bachelor's degree in Electronics & Information Engineering from Huazhong University of Science and Technology (<a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#002FA7">HUST</a>), where he was fortunate to be advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.
</p>
His research centers on deep learning, computer vision, multi-task and multi-modal learning, with a specific focus on (i) efficient learning architectures, (ii) unified visual perception/understanding/generation frameworks, and (iii) optimizers in the era of (M)LLMs. He has been recognized as Outstanding/Top Reviewer at several prestigious conferences, such as <a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#002FA7">ECCV 2024</a>, <a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#002FA7">ACM MM 2024</a>, and <a href="http://bmvc2024.org/people/reviewers/" target="_blank" style="color:#002FA7">BMVC 2024</a>.



<A NAME="News"><h2>News</h2></A>
  <!-- <div style="height:200px;overflow-y:auto;background:#ffffff;"> -->
  <div style="height:300px;overflow-y:auto;">
  <ul>
    <li><b> <font color="#9F6821">[2025.04.04]</font> </b> One co-authored paper on <i>visual generation&representation</i>, <i><a href="https://huggingface.co/papers/2504.00999" target="_blank"> MergeVQ</a></i>, is accepted at <b><a href="https://cvpr.thecvf.com/Conferences/2025" target="_blank" style="color:#000000">CVPR 2025</a></b>, <a href="https://huggingface.co/papers/date/2025-04-03" target="_blank" style="color:#002FA7"><b>ranking 1st</b> in ðŸ¤—Hugging Face Daily Papers,</a> <a href="https://huggingface.co/papers/week/2025-W14" target="_blank" style="color:#002FA7">and <b>ranking 4th</b> in Weekly Papers</a>. Congrats to all co-authors! </li></li>
    <li><b> <font color="#9F6821">[2024.12.12]</font> </b> One co-authored paper on <i>low-level vision</i>, <i><a href="https://arxiv.org/abs/2503.01136" target="_blank"> PGHHNet</a></i>, is accepted at <b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI 2025</a></b>. Congrats to all co-authors!</li></li>
    <li><b> <font color="#9F6821">[2024.11.04]</font> </b> I was recognized as <i>Outstanding Reviewer</i> at <b><a href="https://bmvc2024.org/people/reviewers/" target="_blank" style="color:#000000">BMVC 2024 <font color="#b80000">(rate: 19.3%, 166/860)</font></a></b>. Happy to contribute to the research community! Huge thanks to the BMVC'24 organizers for their dedication.</li></li>
    <li><b> <font color="#9F6821">[2024.11.04]</font> </b> I was recognized as <i>Outstanding Reviewer</i> at <b><a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#000000">ACM MM 2024 <font color="#b80000">(rate: 139/x)</font></a></b>. Happy to contribute to the research community! Huge thanks to the MM'24 organizers for their dedication.</li></li>
    <li><b> <font color="#9F6821">[2024.10.10]</font> </b> One paper on <i>vision backbones & optimizers</i>, <i><a href="https://arxiv.org/abs/2410.06373" target="_blank">BOCB</a></i>, is released. Welcome to visit <a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">Hugging Face</a> and <a href="https://bocb-ai.github.io" target="_blank" style="color:#002FA7">Project</a> page.</li></li>
    <li><b> <font color="#9F6821">[2024.09.24]</font> </b> I was recognized as <i>Outstanding Reviewer</i> at <b><a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#000000">ECCV 2024 <font color="#b80000">(rate: 2.7%, 198/7293)</font></a></b>. Happy to contribute to the research community! Huge thanks to the ECCV'24 organizers for their dedication.</li></li>
    <li><b> <font color="#9F6821">[2024.07.14]</font> </b> Maintain an open-source repository on <i>optimizers</i>, <i><a href="https://github.com/tianshijing/Awesome-Optimizers" target="_blank" >Awesome-Optimizers</a></i>, with <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#002FA7">Juanxi Tian</a> and <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>. </li>
    <li><b> <font color="#9F6821">[2024.04.16]</font> </b> One co-authored paper on <i>long sequence modeling</i>, <i><a href="https://arxiv.org/abs/2404.11163" target="_blank" > LongVQ</a></i>, is accepted at <b><a href="https://ijcai24.org" target="_blank" style="color:#000000">IJCAI 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#002FA7">Zicheng Liu</a>! </li></li>
    <li><b> <font color="#9F6821">[2024.01.16]</font> </b> One paper on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" > MogaNet</a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code & weights</a> (220 stars) are released!</li></li>
    <li><b> <font color="#9F6821">[2024.01.16]</font> </b> One co-authored paper on <i>semi-sup learning</i>, <i><a href="https://arxiv.org/abs/2310.03013" target="_blank" > SemiReward</a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>! </li></li>
    <li><b> <font color="#9F6821">[2024.01.09]</font> </b> I am invited to serve as an <i> emergency reviewer</i> at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b> <b><a href="https://iclr.cc/Conferences/2024/CallForTinyPapers" target="_blank" style="color:#000000">(TinyPapers)</a></b>. It will be held in Vienna, Austria. </li></li>
    <li><b> <font color="#9F6821">[2023.12.31]</font> </b> One co-authored preprint on <i>self-supervised learning</i>, <i><a href="https://arxiv.org/abs/2401.00897" target="_blank" > Masked Modeling on Vision and Beyond</a></i> is released.</li></li>
    <li><b> <font color="#9F6821">[2023.09.31]</font> </b> One co-authored paper on <i>video prediction</i>, <i><a href="https://arxiv.org/abs/2306.11249" target="_blank" > OpenSTL</a></i>, is accepted at <b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">NeurIPS 2023</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=6kTV6aMAAAAJ" target="_blank" style="color:#002FA7">Cheng Tan</a>! </li></li>
    <li><b> <font color="#9F6821">[2023.06.25]</font> </b> Got my B.Eng. degree from Huazhong University of Science and Technology! Special thanks to my undergraduate supervisor Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a> for his generous support!</li>
    <li><b> <font color="#9F6821">[2023.05.23]</font> </b> One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2111.15454" target="_blank" > SAMix</a></i>, is presented for both SL & SSL scenarios. </li>
    <li><b> <font color="#9F6821">[2022.11.07]</font> </b> One preprint on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" > MogaNet</a></i>. A new family of pure convolutional architecture covering <b>5M~100M+</b> model scales with great performance.  <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code & weights</a> are released (220 stars). Welcome to discuss, use, and star!</li>
    <li><b> <font color="#9F6821">[2022.09.11]</font> </b> One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2209.04851" target="_blank" >OpenMixup</a></i>, is presented for vision tasks. This is also my first arXiv paper!</li>
    <li><b> <font color="#9F6821">[2022.09.11]</font> </b> Maintain an open-source repository, <i><a href="https://github.com/Westlake-AI/openmixup" target="_blank" >OpenMixup</a></i> (648 stars), for both supervised, semi- and self-supervised visual representation learning based on PyTorch. On updating! </li>
    <li><b> <font color="#9F6821">[2022.07.06]</font> </b> Fortunate to become <i>visiting student</i> at Westlake University. </li></li>
    <li><b> <font color="#9F6821">[2021.09.01]</font> </b> Fortunate to become <i>research intern</i> under the supervision of Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>. </li></li>
    <li><b> <font color="#9F6821">[2021.06.01]</font> </b> Fortunate to become <i>research intern</i> in <a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#002FA7"> SIAT-MMLab</a> at Shenzhen Institute of Advanced Technology, CAS. </li></li>

</ul>
</div>


<A NAME="Interests"><h2>Research Interests</h2></A>
Currently, I focus mostly on Computer Vision, Multi-task and Multi-modal Learning, including (but not limited to):
<ul>
    <li>Data-efficient Learning: Mixup Augmentation [<a href="https://arxiv.org/abs/2209.04851" target="_blank" style="color:#002FA7">OpenMixup</a>, <a href="https://arxiv.org/abs/2111.15454" target="_blank" style="color:#002FA7">SAMix</a>], Semi-supervised Learning [<a href="https://arxiv.org/abs/2310.03013" target="_blank" style="color:#002FA7">SemiReward</a>].</li>
    <li>Efficient Network Architectures: Vision Backbones [<a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#002FA7">MogaNet</a>], Multi-task Architectures [<a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#002FA7">MergeVQ</a>].</li>
    <li>Deep Learning Optimization: Optimizers in the era of (M)LLMs [<a target="_blank" style="color:#002FA7">BOCB</a>, SGG], Multi-task Optimization.</li>
</ul>


<style>
    p[style*="margin-left: 1.9em;"],
    p[style*="margin-left: 1.7em;"],
    p[style*="margin-left: 1.2em;"],
    p[style*="margin-left: 1.0em;"] {
      border-radius: 8px;
      padding: 11px;
      margin-bottom: 20px;
      transition: all 0.3s ease;
    }
    
    
    
    
    p[style*="margin-left"] img:hover {
      transform: scale(1.05);
    }
    
</style>


<A NAME="Education"><h2>Education</h2></A>
<p style="line-height: 0.05;">&nbsp; </p>

<p style="margin-left: 1.9em;"> 
    <img src="Files/HKUST_logo.png" align="left" width="55" height="78"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://hkust.edu.hk" target="_blank" style="color:#000000">The Hong Kong University of Science and Technology</a> (2025-now) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; Ph.D. in Computer Science and Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Multi-task and Multi-modal Learning. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>.<br>
</p>

<p style="margin-left: 1.0em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="67"/>
    &nbsp&nbsp&nbsp&nbsp <b><a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#000000">Huazhong University of Science and Technology</a> (2019-2023) </b><br> 
    &nbsp&nbsp&nbsp&nbsp &bull; B.Eng. in Electronic and Information Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Thesis:</i> Efficient ConvNet-based Vision Backbone for Multiple Tasks (Grade: 92/100). <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.<br>
</p>

<p style="line-height: 0.05;">&nbsp; </p>
<A NAME="Internship"><h2>Research Experience</h2></A>
<p style="line-height: 0.05;">&nbsp; </p>

<p style="margin-left: 1.7em;"> 
    <img src="Files/ZEEKR_logo.png" align="left" width="65" height="65"/>
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://en.wikipedia.org/wiki/Zeekr" target="_blank" style="color:#000000">ZEEKR Intelligent Technology</a> (2024.04-2025.04) </b><br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Efficient Multi-task Optimization. <br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a> (University-Enterprise Cooperation). <br>
</p>


<p style="margin-left: 1.9em;"> 
    <img src="Files/westlake.ico" align="left" width="62" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#000000">Stan Z. Li's AI Lab, Westlake University</a> (2022.07-2024.02) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Visual Representation Learning and AI for Science. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> Chair Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Stan Z. Li</a> (IEEE Fellow, IAPR Fellow).<br>
</p>


<p style="margin-left: 1.2em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="65"/>
    &nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#000000">Xinggang Wang's Vision Group, HUST</a> (2021.09-2023.03) </b><br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Few-shot Semantic Segmentation. <br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.<br>
</p>


<p style="margin-left: 1.9em;"> 
    <img src="Files/SIAT_logo.png" align="left" width="64" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; <b><a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#000000">SIAT-MMLab, Chinese Academy of Sciences</a> (2021.06-2021.09) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Topics:</i> Semantic Segmentation and Text Spotting. <br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Advisor:</i> Dr. <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#002FA7">Bin Fu</a>.<br>
</p>

<p style="line-height: 0.05;">&nbsp; </p>

<A NAME="Publications"><h2>Publications</h2></A>



<style>
.imgtable {
  margin-bottom: 25px;
  transition: all 0.3s ease;
  border-radius: 12px;
}

.imgtable:hover {
  background-color: #fafaff;
}

.pub-img {
    transition: transform 0.3s ease, box-shadow 0.3s ease;
    border: 1px solid #464646 !important;
    padding: 5px !important;
    border-radius: 12px !important;
    box-shadow: 2px 2px 10px rgba(0,0,0,0.1) !important;
}
    
.pub-img:hover {
  transform: scale(1.03);
  box-shadow: 0 12px 24px rgba(0,0,0,0.15) !important;
}

.pub-title {
    font-size: 14.4px; /* Slightly larger than normal text */
    line-height: 1.3;
    font-weight: bold;
    font-family: Georgia, serif;
    margin-bottom: 3px;
    color: #002FA7;
}
.imgtable td {
  padding: 12px;
  vertical-align: top;
}
</style>



<p><b>Selected Preprints (*: Equal Contribution. â€ : Corresponding Author.)</b>: </p>
<font size="3"> 
<ul>
<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_BOCB.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a target="_blank" style="color:#b80000">[NEW!]</a> <a href= "https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning</a></div>
        <i> Siyuan Li*, Juanxi Tian*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u>*</a></b>, Luyuan Zhang, Zicheng Liu, Weiyang Jin, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2410.06373" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">ðŸ¤—HF</a>]
        [<a href="https://bocb-ai.github.io" target="_blank" style="color:#002FA7">Project</a>]
        [<a href="https://github.com/Black-Box-Optimization-Coupling-Bias/BOCB" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_SEMA.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href= "https://huggingface.co/papers/2402.09240" target="_blank" style="color:#002FA7">Switch EMA: A Free Lunch for Better Flatness and Sharpness</a></div>
        <i> Siyuan Li*, Zicheng Liu*, Juanxi Tian*, Ge Wang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2402.09240" target="_blank" style="color:#002FA7">ðŸ¤—HF</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2023_MM_Survey.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#002FA7">Masked Modeling for Self-supervised Representation Learning on Vision and Beyond</a></div>
        <i> Siyuan Li*, Luyuan Zhang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Di Wu, Lirong Wu, Zicheng Liu, Jun Xia, Cheng Tan, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2023</b></i><br>
        [<a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Lupin1998/Awesome-MIM" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2023_MIMSurvey_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>



</ul>
<br />


<p><b>Conferences (As First Author)</b>: </p>
<font size="3"> 
<ul>


<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_MogaNet.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href= "https://iclr.cc/virtual/2024/poster/18447" target="_blank" style="color:#002FA7">MogaNet: Multi-order Gated Aggregation Network</a></div>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u>*</a></b>, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Liâ€  </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2211.03295" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/582542948" target="_blank" style="color:#002FA7">Zhihu</a>]
        [<a href="./Files/iclr_2024_MogaNet_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_VQDNA.jpeg" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href="https://proceedings.mlr.press/v235/li24bm.html" target="_blank" style="color:#002FA7">VQDNA: Unleashing the Power of Vector Quantization for Genomic Sequence Modeling</a></div>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u>*</a></b>, Zicheng Liu, Di Wu, Cheng Tan, Jiangbin Zheng, Yufei Huang, Stan Z. Liâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Lupin1998/VQDNA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/icml_2024_VQDNA_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

</ul>

<p><b>Conferences (As Co-author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Teasers/CVPR25_MergeVQ.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a target="_blank" style="color:#b80000">[NEW!]</a><a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#002FA7">MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization</a></div>
        <i>Siyuan Li*, Luyuan Zhang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Juanxi Tian, Cheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie, Haonan Lu, Haoqian Wang, Zhen Leiâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">IEEE/CVF Conference on Computer Vision (CVPR), 2025</a></b></i><br>
        [<a href="https://arxiv.org/abs/2504.00999" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#9F6821">ðŸ¤—<b>HF DailyPapers Top-1</b></a>]
        [<a href="https://apexgen-x.github.io/MergeVQ/" target="_blank" style="color:#002FA7">Project</a>]
        [<a href="./Files/cvpr_2025_MergeVQ_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_CHELA.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href="https://proceedings.mlr.press/v235/liu24ak.html" target="_blank" style="color:#002FA7">Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences</a></div>
        <i> Zicheng Liu, Siyuan Li, Li Wang, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Yunfan Liu, Stan Z. Liâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/icml_2024_CHELA_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/IJCAI24_LongVQ.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href="https://www.ijcai.org/proceedings/2024/510" target="_blank" style="color:#002FA7">LongVQ: Long Sequence Modeling with Vector Quantization on Structured Memory</a></div>
        <i> Zicheng Liu, Li Wang, Siyuan Li, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Haitao Lin, Stan Z. Liâ€  </a></i><br><i><b><a href="https://ijcai24.org" target="_blank" style="color:#000000">International Joint Conference on Artificial Intelligence (IJCAI), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/ijcai_2024_LongVQ_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>
    

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_SemiReward.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href= "https://iclr.cc/virtual/2024/poster/18245" target="_blank" style="color:#002FA7">SemiReward: A General Reward Model for Semi-supervised Learning</a></div>
        <i> Siyuan Li*, Weiyang Jin*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Liâ€  </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2310.03013" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/SemiReward" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/iclr_2024_SemiReward_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/NeurIPS23_OpenSTL.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href= "https://papers.nips.cc/paper_files/paper/2023/hash/dcbff44d11130e75d09d3930411c23e1-Abstract-Datasets_and_Benchmarks.html" target="_blank" style="color:#002FA7">OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning</a></div>
        <i> Cheng Tan*, Siyuan Li*, Zhangyang Gao, Wenfei Guan, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000"><u>Zedong Wang</u></a></b>, Zicheng Liu, Lirong Wu, Stan Z. Liâ€  </a></i><br><i><b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">Advances in Neural Information Processing Systems (NeurIPS), 2023</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2306.11249" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#002FA7">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/640271275" target="_blank" style="color:#002FA7">Zhihu</a>]
        [<a href="./Files/NIPS_2023_OpenSTL_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>   


</ul>


<A NAME="Services"><h2>Professional Services</h2></A>

<p><b>Conference Reviewer/PC Member:</b> </p>
<font size="3"> 
<ul>

<li>International Conference on Learning Representations (<b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR</a></b>), 2024 (TinyPapers), 2025<br /></li>
<li>Annual Conference on Neural Information Processing Systems (<b><a href="https://neurips.cc" target="_blank" style="color:#000000">NeurIPS</a></b>), 2024, 2025<br /></li>
<li>International Conference on Machine Learning (<b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML</a></b>), 2024, 2025<br /></li>
<li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b><a href="https://cvpr.thecvf.com" target="_blank" style="color:#000000">CVPR</a></b>), 2025<br /></li>
<li>European Conference on Computer Vision (<b><a href="https://eccv.ecva.net" target="_blank" style="color:#000000">ECCV</a></b>), 2024<br /></li>
<li>AAAI Conference on Artificial Intelligence (<b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI</a></b>), 2025<br /></li>
<li>ACM International Conference on Multimedia (<b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM</a></b>), 2024<br /></li>
<li>BMVA The British Machine Vision Conference (<b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC</a></b>), 2024, 2025<br /></li>
<li>IAPR International Conference on Pattern Recognition (<b><a href="https://icpr2024.org" target="_blank" style="color:#000000">ICPR</a></b>), 2024<br /></li>
</ul>
</font>
<br />
 
<p><b>Journal Reviewer:</b> </p>
<font size="3"> 
<ul>
<li>IEEE Transactions on Knowledge and Data Engineering (<b><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69" target="_blank" style="color:#000000">TKDE</a></b>)</li>
<li>IEEE Transactions on Big Data (<b><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6687317" target="_blank" style="color:#000000">TBD</a></b>)</li>
</ul>
</font>
<br />

<p><b>Membership</b>: </p>
<font size="3"> 
<ul>
<li>China Computer Federation (<b><a href="https://www.ccf.org.cn/en/" target="_blank" style="color:#000000">CCF</a></b>), Student Member, 2024-2026</li>
<li>China Society of Image and Graphics (<b><a href="https://en.csig.org.cn" target="_blank" style="color:#000000">CSIG</a></b>), Student Member, 2023</li>
</ul>
</font>
<br />


<A NAME="Awards"><h2>Selected Awards and Honors</h2></A>
<p style="line-height: 0.1;">&nbsp; </p>

<b>Outstanding/Top Reviewer Nomination: </b>
<ul>
    <li>The 18th European Conference on Computer Vision <a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#002FA7"><b>(ECCV 2024)</b></a> <b><font color="#b80000"> [rate: 2.7%, 198/7293]</font></b>. </li>
    <li>The 32nd ACM International Conference on Multimedia <a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#002FA7"><b>(ACM MM 2024)</b></a> <b><font color="#b80000"> [rate: 139/X]</font></b>. </li>
    <li>The 35th British Machine Vision Conference <a href="https://bmvc2024.org/people/reviewers/" target="_blank" style="color:#002FA7"><b>(BMVC 2024)</b></a> <b><font color="#b80000"> [rate: 19.3%, 166/860]</font></b></li>
</ul>


<A NAME="Acknowledgement"><h2>Acknowledgement</h2></A>
My research career cannot be possible without the support from all my awesome mentors, collaborators, and friends:
<ul>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>, Prof. <a href="https://yuzhou.vlrlab.net" target="_blank" style="color:#002FA7">Yu Zhou</a> at HUST.</li>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Stan Z. Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#002FA7">Zicheng Liu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=o5A23qIAAAAJ" target="_blank" style="color:#002FA7">Haitao Lin</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=egz8bGQAAAAJ" target="_blank" style="color:#002FA7">Jiangbin Zheng</a>, Mr. Siqi Ma at Westlake University.</li>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=ennCRJAAAAAJ" target="_blank" style="color:#002FA7">Zhenxing Mi</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=4OuUHYQAAAAJ" target="_blank" style="color:#002FA7">Yuxin Wang</a>, Mr. <a href="https://wangandyyucheng.github.io" target="_blank" style="color:#002FA7">Yucheng Wang</a>, Mr. <a href="https://scholar.google.com/citations?hl=en&user=1xsk9rYAAAAJ" target="_blank" style="color:#002FA7">Yiwei Chen</a> at HKUST.</li>
<li>Mr. <a href="https://scholar.google.com/citations?hl=en&user=Ww37Oi4AAAAJ" target="_blank" style="color:#002FA7">Aozhong Zhang</a> at SUNY Albany.</li>
</ul>
<div style="text-align:justify">
<p style="line-height: 0.2;">&nbsp; </p>
Beyond academia, I feel incredibly fortunate to met wonderful friends along the way (particularly during my middle and high school years in Shenzhen and two years in Hangzhou). Special thanks to Chloe, we met by chance and both joined Westlake University for summer research in 2022. Thank you for your presence and support these years, and I am deeply grateful for our shared moments. Whether we are still in touch or not, I wish you all the best and a bright future!

<p style="line-height: 0.1;">&nbsp; </p>
Even if most of you above may never visit this page, my door is always open for a chat - coffee's on me! 
</div>
<br />



<A NAME="Fragments of Memories"><h2>Fragments of Memories</h2></A>
Great memories with my advisors: Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a> (HUST). 
</p>

<img src="./Files/Zedong_With_Prof_Xinggang.jpeg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="190px" />&nbsp;</td>


<br />

<A NAME=""><h2></h2></A>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=305&t=n&d=p1SE8mvpEDXWAl_SKZ3NCPEvAjvDm5pAjUQB-nDQa5U&cmo=edbf4a&cmn=ff5f5f&co=306285'></script>

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<style>
    #back_top {
      position: fixed;
      bottom: 30px;
      right: 30px;
      width: 40px;
      height: 40px;
      background-color: #002FA7;
      border-radius: 50%;
      color: white;
      display: none;
      justify-content: center;
      align-items: center;
      cursor: pointer;
      box-shadow: 0 2px 10px rgba(0,0,0,0.2);
      transition: all 0.3s ease;
    }
    
    #back_top:hover {
      background-color: #001F6D;
      transform: translateY(-3px);
    }
    
    #back_top:before {
      content: "â–²";
      font-size: 18px;
    }
    </style>
    
    <!-- Replace the existing back_top div with: -->
    <div id="back_top"></div>
    
    <script>
    $(function(){
        $(window).scroll(function(){
            var scrollt = document.documentElement.scrollTop + document.body.scrollTop;
            if(scrollt > 400) {
                $("#back_top").fadeIn(400).css("display", "flex");
            } else {
                $("#back_top").stop().fadeOut(400);
            }
        });
    
        $("#back_top").click(function(){ 
            $("html,body").animate({scrollTop:"0px"}, 200);
        }); 
    });
    </script>

<div style="text-align:center; margin-top:20px; color:#A0A0A0; font-size:12px;">
    Last Updated: <span id="lastUpdated"></span>
  </div>
  
  <script>
    document.getElementById('lastUpdated').textContent = new Date().toLocaleDateString('en-US', { 
      year: 'numeric', 
      month: 'long', 
      day: 'numeric' 
    });
</script>

<!-- <font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2022.06.01</p>
</font> -->
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;All Rights Reserved by Zedong Wang. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.

</body>
</html>
