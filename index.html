<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Zedong Wang, çŽ‹æ³½æ ‹, Deep learning, HKUST">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/HKUST_logo.png">
<title>Zedong Wang</title>
<script async defer src="https://buttons.github.io/buttons.js"></script>

</head>


<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./" style="color:#002FA7"><img src="./Files/Profile_Twitter.jpeg" alt="" height="222px" /></a>&nbsp;</td>
<td align="left"><p><a href="./" style="color:#000000"><font size="5">WANG, Ze Dong </font></a><br />
<i> <a href="https://cse.hkust.edu.hk/admin/factsheet/" target="_blank" style="color:#002FA7">Department of Computer Science and Engineering (CSE)</a> </i>
<br />
<i> <a href="https://hkust.edu.hk" target="_blank" style="color:#002FA7">The Hong Kong University of Science and Technology (HKUST)</a> </i>

<br /><br />
Location: Academic Building, HKUST, Clear Water Bay, Kowloon, Hong Kong (<a href="https://www.google.com/maps/search/HKUST+map/@22.335677,114.2613186,17z" target="_blank" style="color:#002FA7">MAP</a>).<br />


<class="staffshortcut">
 <A HREF="#News" style="color:#002FA7">News</A> | 
 <A HREF="#Interests" style="color:#002FA7">Interests</A> | 
 <A HREF="#Education" style="color:#002FA7">Education</A> | 
 <A HREF="#Internship" style="color:#002FA7">Intern</A> | 
 <A HREF="#Publications" style="color:#002FA7">Publications</A> | 
 <A HREF="#Services" style="color:#002FA7">Services</A> | 
 <A HREF="#Awards" style="color:#002FA7">Awards</A> | 
 <A HREF="#Acknowledgement" style="color:#002FA7">Notes</A> | 
 <A HREF="./Files/Jacky_CV.pdf" style="color:#002FA7">ResumÃ© (CV)</A> 
<br />
<br />

Email: zedong.wang@connect.ust.hk (preferred); zwangmw@cse.ust.hk; wangzedong@westlake.edu.cn;<br />
[<a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#002FA7">Google Scholar</a>]  
[<a href="https://openreview.net/profile?id=~Zedong_Wang1" target="_blank" style="color:#002FA7">OpenReview</a>]
[<a href="https://twitter.com/ZedongWangAI" target="_blank" style="color:#002FA7">Twitter (X)</a>]
[<a href="https://huggingface.co/ZedongWangAI" target="_blank" style="color:#002FA7">Hugging Face</a>] 
[<a href="https://github.com/Jacky1128" target="_blank" style="color:#002FA7">GitHub</a>] 
[<a href="https://orcid.org/0009-0000-0112-0491" target="_blank" style="color:#002FA7">ORCID</a>] 
[<a href="https://www.threads.net/@jackyw_28" target="_blank" style="color:#002FA7">Threads</a>] 


<br />
<br />
<i>Welcome to contact me about research or internship on related topics, including multi-task learning, efficient network architectures, and optimization techniques. Feel free to drop me emails. </i><br />
</td></tr></table>



<A NAME="Short Bio"><h2>Short Bio</h2></A>
<div style="text-align:justify; line-height:136%">
Zedong Wang (çŽ‹æ¾¤æ£Ÿ) is currently a first-year Ph.D. student in Department of Computer Science and Engineering (<a href="https://cse.hkust.edu.hk/admin/factsheet/" target="_blank" style="color:#002FA7">CSE</a>) at The Hong Kong University of Science and Technology (<a href="https://hkust.edu.hk" target="_blank" style="color:#002FA7">HKUST</a>), 
advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>. Previously, he was a visiting student at <a href="https://en.westlake.edu.cn" target="_blank" style="color:#002FA7">Westlake University</a>, 
advised by Chair Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Stan Z. Li</a>.
He received his bachelor's degree in Electronics and Information Engineering from Huazhong University of Science and Technology (<a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#002FA7">HUST</a>) in 2023, where he was fortunately advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.
</p>
He has been recognized as Outstanding Reviewer at several prestigious conferences, such as <a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#002FA7">ECCV'24</a>, <a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#002FA7">MM'24</a>, and <a href="http://bmvc2024.org/people/reviewers/" target="_blank" style="color:#002FA7">BMVC'24</a>. His research centers on computer vision and multi-task learning, with a specific focus on fresh/efficient network architectures, learning paradigms, and optimization techniques, as well as their applications in autonomous driving and more.





<A NAME="News"><h2>News</h2></A>
  <!-- <div style="height:200px;overflow-y:auto;background:#ffffff;"> -->
  <div style="height:250px;overflow-y:auto;">
  <ul>
    <li><b> <font color="#9F6821">[2025.02.14]</font> </b> I am invited to serve as a <i>Reviewer</i> at <b><a href="https://iccv.thecvf.com" target="_blank" style="color:#000000">ICCV 2025</a></b>, which will be held in Honolulu, Hawaii, USA.</li></li>
    <li><b> <font color="#9F6821">[2024.12.12]</font> </b> One co-authored paper on <i>low-level vision</i> is accepted at <b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI 2025</a></b>.</li></li>
    <li><b> <font color="#9F6821">[2024.12.12]</font> </b> I am invited to serve as a <i>Reviewer</i> at <b><a href="https://icml.cc/Conferences/2025" target="_blank" style="color:#000000">ICML 2025</a></b>, which will be held in Vancouver, Canada.</li></li>
    <li><b> <font color="#9F6821">[2024.11.04]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> at <b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC 2024 <font color="#b80000">(rate: 19.3%, 166/860)</font></a></b>. Happy to contribute to the community! Huge thanks to the BMVC organizers for their dedication.</li></li>
    <li><b> <font color="#9F6821">[2024.11.04]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> at <b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM 2024 <font color="#b80000">(rate: 139/X)</font></a></b>. Happy to contribute to the community! Huge thanks to the ACM MM organizers for their dedication.</li></li>
    <li><b> <font color="#9F6821">[2024.11.04]</font> </b> I am invited to serve as a <i>Reviewer</i> at <b><a href="https://cvpr.thecvf.com/Conferences/2025" target="_blank" style="color:#000000">CVPR 2025</a></b>, which will be held in Nashville TN, USA.</li></li>
    <li><b> <font color="#9F6821">[2024.10.10]</font> </b> One paper on <i>vision backbones & optimizers</i>, <i><a href="https://arxiv.org/abs/2410.06373" target="_blank"> BOCB</a></i>, is released. Welcome to visit the <a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">HF</a> and <a href="https://bocb-ai.github.io" target="_blank" style="color:#002FA7">Project</a> page for further discussion! Happy to explore something interesting together with <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#002FA7">Juanxi Tian</a> and <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>!</li></li>
    <li><b> <font color="#9F6821">[2024.09.24]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> at <b><a href="https://eccv.ecva.net/Conferences/2024" target="_blank" style="color:#000000">ECCV 2024 <font color="#b80000">(rate: 2.7%, 198/7293)</font></a></b>. Happy to contribute to the community! Huge thanks to the ECCV organizers for their dedication.</li></li>
    <li><b> <font color="#9F6821">[2024.08.15]</font> </b> I am invited to serve as a <i>Reviewer</i> at <b><a href="https://iclr.cc/Conferences/2025" target="_blank" style="color:#000000">ICLR 2025</a></b>, which will be held in Singapore.</li></li>
    <li><b> <font color="#9F6821">[2024.07.20]</font> </b> I am invited to serve as <i>Program Committee Member</i> at <b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI 2025</a></b>, which will be held in Philadelphia, USA. </li></li>
    <li><b> <font color="#9F6821">[2024.07.14]</font> </b> Maintain an open-source repository on <i>optimizers</i>, <i><a href="https://github.com/tianshijing/Awesome-Optimizers" target="_blank" >Awesome-Optimizers</a></i>, with <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#002FA7">Juanxi Tian</a> and <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>. </li>
    <li><b> <font color="#9F6821">[2024.06.04]</font> </b> I am invited to serve as a <i>Reviewer</i> at <b><a href="https://neurips.cc/Conferences/2024/CallForDatasetsBenchmarks" target="_blank" style="color:#000000">NeurIPS 2024 (Datasets and Benchmarks Track)</a></b>. </li></li>
    <li><b> <font color="#9F6821">[2024.05.02]</font> </b> One paper on <i>AI4Science (genomics)</i>, <i><a href="https://arxiv.org/abs/2405.10812" target="_blank" > VQDNA</a></i>, is accepted at <b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML 2024</a></b>. </li></li>
    <li><b> <font color="#9F6821">[2024.05.02]</font> </b> One co-authored paper on <i>long sequence modeling</i>, <i><a href="https://arxiv.org/abs/2406.08128" target="_blank" > CHELA</a></i>, is accepted at <b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#002FA7">Zicheng Liu</a>! </li></li>
    <li><b> <font color="#9F6821">[2024.05.01]</font> </b> I am invited to serve as a <i>Reviewer</i> at <b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC 2024</a></b>, which will be held in Glasgow, UK.</li></li>
    <li><b> <font color="#9F6821">[2024.04.16]</font> </b> One co-authored paper on <i>long sequence modeling</i>, <i><a href="https://arxiv.org/abs/2404.11163" target="_blank" > LongVQ</a></i>, is accepted at <b><a href="https://ijcai24.org" target="_blank" style="color:#000000">IJCAI 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#002FA7">Zicheng Liu</a>! </li></li>
    <li><b> <font color="#9F6821">[2024.04.13]</font> </b> I am invited to serve as a <i>Reviewer</i> at <b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM 2024</a></b>, which will be held in Melbourne, Australia. </li></li>
    <li><b> <font color="#9F6821">[2024.03.12]</font> </b> I am invited to serve as a <i>Reviewer</i> at <b><a href="https://eccv.ecva.net/Conferences/2024" target="_blank" style="color:#000000">ECCV 2024</a></b>, which will be held in MiCo Milano, Italy.</li></li>
    <li><b> <font color="#9F6821">[2024.01.22]</font> </b> I am invited to serve as a <i>Reviewer</i> at <b><a href="https://icml.cc/Conferences/2024" target="_blank" style="color:#000000">ICML 2024</a></b>, which will be held in Vienna, Austria.</li></li>
    <li><b> <font color="#9F6821">[2024.01.17]</font> </b> I am invited to serve as a <i>Reviewer</i> at <b><a href="https://icpr2024.org" target="_blank" style="color:#000000">ICPR 2024</a></b>, which will be held in Kolkata, India.</li></li>
    <li><b> <font color="#9F6821">[2024.01.16]</font> </b> One paper on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" > MogaNet</a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code & weights</a> (180 stars) are released!</li></li>
    <li><b> <font color="#9F6821">[2024.01.16]</font> </b> One co-authored paper on <i>semi-sup learning</i>, <i><a href="https://arxiv.org/abs/2310.03013" target="_blank" > SemiReward</a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>! </li></li>
    <li><b> <font color="#9F6821">[2024.01.09]</font> </b> I am invited to serve as an <i> emergency reviewer</i> at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b> <b><a href="https://iclr.cc/Conferences/2024/CallForTinyPapers" target="_blank" style="color:#000000">(TinyPapers)</a></b>. It will be held in Vienna, Austria. </li></li>
    <li><b> <font color="#9F6821">[2023.12.31]</font> </b> One co-authored preprint on <i>self-supervised learning</i>, <i><a href="https://arxiv.org/abs/2401.00897" target="_blank" > Masked Modeling on Vision and Beyond</a></i>. </li></li>
    <li><b> <font color="#9F6821">[2023.09.31]</font> </b> One co-authored paper on <i>video prediction</i>, <i><a href="https://arxiv.org/abs/2306.11249" target="_blank" > OpenSTL</a></i>, is accepted at <b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">NeurIPS 2023</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=6kTV6aMAAAAJ" target="_blank" style="color:#002FA7">Cheng Tan</a>! </li></li>
    <li><b> <font color="#9F6821">[2023.06.25]</font> </b> Got my B.Eng. degree from Huazhong University of Science and Technology (HUST)! Special thanks to my undergraduate supervisor <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Prof. Xinggang Wang</a> for the generous support!</li>
    <li><b> <font color="#9F6821">[2023.05.23]</font> </b> One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2111.15454" target="_blank" > SAMix</a></i>, is presented for both SL & SSL scenarios. </li>
    <li><b> <font color="#9F6821">[2022.11.07]</font> </b> One preprint on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" > MogaNet</a></i>. A new family of pure convolutional architecture covering <b>5M~100M+</b> model scales with great performance.  <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code & weights</a> are released (180 stars). Welcome to discuss, use, and star!</li>
    <li><b> <font color="#9F6821">[2022.09.11]</font> </b> One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2209.04851" target="_blank" >OpenMixup</a></i>, is presented for vision tasks. This is also my first arXiv paper!</li>
    <li><b> <font color="#9F6821">[2022.09.11]</font> </b> Maintain an open-source repository, <i><a href="https://github.com/Westlake-AI/openmixup" target="_blank" >OpenMixup</a></i> (618 stars), for both supervised, semi- and self-supervised visual representation learning based on PyTorch. On updating! </li>
    <li><b> <font color="#9F6821">[2022.07.06]</font> </b> Fortunate to become <i>visiting student</i> at Westlake University, under the supervision of <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Chair Prof. Stan Z. Li</a>. </li></li>
    <li><b> <font color="#9F6821">[2021.09.01]</font> </b> Fortunate to become <i>research intern</i> in <a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#002FA7"> HUST Vision Lab</a>, under the supervision of <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Prof. Xinggang Wang</a>. </li></li>
    <li><b> <font color="#9F6821">[2021.06.01]</font> </b> Fortunate to become <i>research intern</i> in <a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#002FA7"> SIAT-MMLab</a> at Shenzhen Institute of Advanced Technology, CAS. </li></li>

</ul>
</div>


<A NAME="Interests"><h2>Research Interests</h2></A>
Currently, I focus mostly on Computer Vision and Efficient Multi-task Learning, including (but not limited to):
<ul>
    <li>Data-efficient Learning: Mixup Augmentation [<a href="https://arxiv.org/abs/2209.04851" target="_blank" style="color:#002FA7">OpenMixup</a>, <a href="https://arxiv.org/abs/2111.15454" target="_blank" style="color:#002FA7">SAMix</a>], Semi-supervised Learning [<a href="https://arxiv.org/abs/2310.03013" target="_blank" style="color:#002FA7">SemiReward</a>].</li>
    <li>Efficient Network Architectures: Vision Backbones [<a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#002FA7">MogaNet</a>], Multi-task Architectures [MergeVQ].</li>
    <li>Optimization Techniques: Optimizers [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#002FA7">SEMA</a>, <a target="_blank" style="color:#002FA7">BOCB</a>], Multi-task Optimization.</li>
    <li>Downstream Applications: End-to-End Autonomous Driving.</li>
</ul>


<A NAME="Education"><h2>Education</h2></A>

<p style="line-height: 0.2;">&nbsp; </p>

<p style="margin-left: 1.9em;"> 
    <img src="Files/HKUST_logo.png" align="left" width="55" height="78"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://hkust.edu.hk" target="_blank" style="color:#000000">The Hong Kong University of Science and Technology</a> (2025-now) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; Ph.D. in Computer Science and Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Efficient Multi-task Learning. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Supervisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>.<br>
</p>
<p style="line-height: 0.4;">&nbsp; </p>

<p style="margin-left: 1.0em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="67"/>
    &nbsp&nbsp&nbsp&nbsp <b><a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#000000">Huazhong University of Science and Technology</a> (2019-2023) </b><br> 
    &nbsp&nbsp&nbsp&nbsp &bull; B.Eng. in Electronic and Information Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Thesis:</i> Efficient ConvNet-based Vision Backbone for Multiple Tasks (Grade: 92/100). <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Supervisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.<br>
</p>
<p style="line-height: 0.1;">&nbsp; </p>

<A NAME="Internship"><h2>Research Experience</h2></A>
<!--
<ul>
<li>2024.04-now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on multi-task perception at <a href="https://en.wikipedia.org/wiki/Zeekr" target="_blank" style="color:#002FA7">ZEEKR, Geely Holding Group</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Prof. Dan Xu</a>.</li>
<li>2024.03-now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp;&thinsp; Research on multi-modal & multi-task learning at <a href="https://hkust.edu.hk" target="_blank" style="color:#002FA7">HKUST</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp;&thinsp;&nbsp;&nbsp;&thinsp;&thinsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Prof. Dan Xu</a>.</li>
<li>2022.06-now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp;&thinsp; Research on representation learning & AI4S at <a href="https://en.westlake.edu.cn" target="_blank" style="color:#002FA7">Westlake University</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Chair Prof. Stan Z. Li</a>.</li>
<li>2021.09-2023.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on few-shot semantic segmentation at <a href="https://www.hust.edu.cn" target="_blank" style="color:#002FA7">HUST Vision Lab</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Prof. Xinggang Wang</a>.</li>
<li>2021.06-2021.09 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp; Research on semantic segmentation & text spotting at <a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#002FA7">SIAT-MMLab</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#002FA7">Dr. Bin Fu</a>.</li>
<li>2020.06-2021.04 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on remote sensing high-res semantic segmentation at <a href="http://www.digitalearthlab.com.cn" target="_blank" style="color:#002FA7">CAS</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=TpX2C3cAAAAJ" target="_blank" style="color:#002FA7">Dr. Xiaoping Du</a>.</li>
</ul>
<br /> 
-->
<p style="line-height: 0.3;">&nbsp; </p>

<p style="margin-left: 1.7em;"> 
    <img src="Files/ZEEKR_logo.png" align="left" width="65" height="65"/>
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://en.wikipedia.org/wiki/Zeekr" target="_blank" style="color:#000000">ZEEKR Intelligent Technology</a> (2024.04-present) </b><br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Efficient Multi-task Learning in Autonomous Driving. <br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a> (University-Enterprise Cooperation). <br>
</p>

<p style="line-height: 0.6;">&nbsp; </p>


<p style="margin-left: 1.9em;"> 
    <img src="Files/westlake.ico" align="left" width="62" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#000000">Stan Z. Li's AI Lab, Westlake University</a> (2022.06-present) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Visual Representation Learning and AI for Genomics. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> Chair Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Stan Z. Li</a> (IEEE Fellow, IAPR Fellow).<br>
</p>

<p style="line-height: 0.6;">&nbsp; </p>

<p style="margin-left: 1.2em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="65"/>
    &nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#000000">Xinggang Wang's Vision Group, HUST</a> (2021.09-2022.06) </b><br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Few-shot Semantic Segmentation. <br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.<br>
</p>

<p style="line-height: 0.6;">&nbsp; </p>

<p style="margin-left: 1.9em;"> 
    <img src="Files/SIAT_logo.png" align="left" width="64" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; <b><a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#000000">SIAT-MMLab, Chinese Academy of Sciences</a> (2021.06-2021.09) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Topics:</i> Semantic Segmentation and Text Spotting. <br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Advisor:</i> Dr. <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#002FA7">Bin Fu</a>.<br>
</p>

<p style="line-height: 0.3;">&nbsp; </p>

<A NAME="Publications"><h2>Publications</h2></A>

<p><b>Selected Preprints (*: Equal Contribution. â€ : Corresponding Author.)</b>: </p>
<font size="3"> 
<ul>
<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_BOCB.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a target="_blank" style="color:#b80000">[NEW!]</a></b> <b><a href= "https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning</a></b></font><br>
        <i> Siyuan Li*, Juanxi Tian*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Luyuan Zhang, Zicheng Liu, Weiyang Jin, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2410.06373" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">ðŸ¤—HF</a>]
        [<a href="https://bocb-ai.github.io" target="_blank" style="color:#002FA7">Project</a>]
        [<a href="https://github.com/Black-Box-Optimization-Coupling-Bias/BOCB" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_SEMA.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://huggingface.co/papers/2402.09240" target="_blank" style="color:#002FA7">Switch EMA: A Free Lunch for Better Flatness and Sharpness</a></b></font><br>
        <i> Siyuan Li*, Zicheng Liu*, Juanxi Tian*, Ge Wang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2402.09240" target="_blank" style="color:#002FA7">ðŸ¤—HF</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2023_MM_Survey.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#002FA7">Masked Modeling for Self-supervised Representation Learning on Vision and Beyond</a></b></font><br>
        <i> Siyuan Li*, Luyuan Zhang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Di Wu, Lirong Wu, Zicheng Liu, Jun Xia, Cheng Tan, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2023</b></i><br>
        [<a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Lupin1998/Awesome-MIM" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2023_MIMSurvey_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2022_OpenMixup.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2209.04851" target="_blank" style="color:#002FA7">OpenMixup: A Comprehensive Mixup Benchmark for Visual Classification</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Zicheng Liu, Di Wu, Cheng Tan, Weiyang Jin, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2022</b></i><br>
        [<a href= "https://arxiv.org/abs/2209.04851" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arxiv_2022_OpenMixup_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv22_SAMix.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2111.15454" target="_blank" style="color:#002FA7">Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup</a></b></font><br>
        <i> Siyuan Li*, Zicheng Liu*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Di Wu, Zihan Liu, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2022</b></i><br>
        [<a href= "https://arxiv.org/abs/2111.15454" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2021_SAMix_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


</ul>
<br />


<p><b>Conferences (As First Author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_VQDNA.jpeg" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#002FA7">VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Zicheng Liu, Di Wu, Cheng Tan, Jiangbin Zheng, Yufei Huang, Stan Z. Liâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Lupin1998/VQDNA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/icml_2024_VQDNA_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_MogaNet.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2211.03295" target="_blank" style="color:#002FA7">MogaNet: Multi-order Gated Aggregation Network</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Liâ€  </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2211.03295" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/582542948" target="_blank" style="color:#002FA7">Zhihu</a>]
        [<a href="./Files/iclr_2024_MogaNet_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

</ul>

<p><b>Conferences (As Co-author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_CHELA.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#002FA7">Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences</a></b></font><br>
        <i> Zicheng Liu, Siyuan Li, Li Wang, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Yunfan Liu, Stan Z. Liâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/icml_2024_CHELA_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/IJCAI24_LongVQ.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#002FA7">LongVQ: Long Sequence Modeling with Vector Quantization on Structured Memory</a></b></font><br>
        <i> Zicheng Liu, Li Wang, Siyuan Li, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Haitao Lin, Stan Z. Liâ€  </a></i><br><i><b><a href="https://ijcai24.org" target="_blank" style="color:#000000">International Joint Conference on Artificial Intelligence (IJCAI), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/ijcai_2024_LongVQ_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>
    

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_SemiReward.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2310.03013" target="_blank" style="color:#002FA7">SemiReward: A General Reward Model for Semi-supervised Learning</a></b></font><br>
        <i> Siyuan Li*, Weiyang Jin*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Liâ€  </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2310.03013" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/SemiReward" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/iclr_2024_SemiReward_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/NeurIPS23_OpenSTL.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2306.11249" target="_blank" style="color:#002FA7">OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning</a></b></font><br>
        <i> Cheng Tan*, Siyuan Li*, Zhangyang Gao, Wenfei Guan, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Zicheng Liu, Lirong Wu, Stan Z. Liâ€  </a></i><br><i><b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">Advances in Neural Information Processing Systems (NeurIPS), 2023</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2306.11249" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#002FA7">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/640271275" target="_blank" style="color:#002FA7">Zhihu</a>]
        [<a href="./Files/NIPS_2023_OpenSTL_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>   


</ul>

<!--
<p><b>Journals</b>: </p>
-->
<font size="3"> 
<ul>

</ul>


<A NAME="Services"><h2>Professional Services</h2></A>

<p><b>Conference Reviewer/PC Member:</b> </p>
<font size="3"> 
<ul>

<li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b><a href="https://cvpr.thecvf.com" target="_blank" style="color:#000000">CVPR</a></b>), 2025<br /></li>
<li>IEEE/CVF International Conference on Computer Vision (<b><a href="https://iccv.thecvf.com" target="_blank" style="color:#000000">ICCV</a></b>), 2025<br /></li>
<li>European Conference on Computer Vision (<b><a href="https://eccv.ecva.net" target="_blank" style="color:#000000">ECCV</a></b>), 2024<br /></li>
<li>International Conference on Learning Representations (<b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR</a></b>), 2024 (TinyPapers), 2025<br /></li>
<li>International Conference on Machine Learning (<b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML</a></b>), 2024, 2025<br /></li>
<li>Conference on Neural Information Processing Systems (<b><a href="https://neurips.cc" target="_blank" style="color:#000000">NeurIPS</a></b>), 2024 (D&B Track)<br /></li>
<li>AAAI Conference on Artificial Intelligence (<b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI</a></b>), 2025<br /></li>
<li>ACM International Conference on Multimedia (<b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM</a></b>), 2024<br /></li>
<li>BMVA The British Machine Vision Conference (<b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC</a></b>), 2024<br /></li>
<li>IAPR International Conference on Pattern Recognition (<b><a href="https://icpr2024.org" target="_blank" style="color:#000000">ICPR</a></b>), 2024<br /></li>
</ul>
</font>
<br />
 
<p><b>Journal Reviewer:</b> </p>
<font size="3"> 
<ul>
<li>IEEE Transactions on Knowledge and Data Engineering (<b><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69" target="_blank" style="color:#000000">TKDE</a></b>)</li>
</ul>
</font>
<br />

<p><b>Membership</b>: </p>
<font size="3"> 
<ul>
<li>China Computer Federation (<b><a href="https://www.ccf.org.cn/en/" target="_blank" style="color:#000000">CCF</a></b>), Student Member, 2024-2026</li>
<li>China Society of Image and Graphics (<b><a href="https://en.csig.org.cn" target="_blank" style="color:#000000">CSIG</a></b>), Student Member, 2023</li>
</ul>
</font>
<br />


<A NAME="Awards"><h2>Selected Awards and Scholarships</h2></A>
<p style="line-height: 0.1;">&nbsp; </p>

<ul>
    <li><b>2024:</b> HKUST Postgraduate Studentship (HK$18,760/month)</li>
    <p style="line-height: 0.1;">&thinsp; </p>
    <li><b>2024:</b> <a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#000000">ECCV 2024 Outstanding Reviewer Award</a> (2.7%, 198/7293). </li>
    <p style="line-height: 0.1;">&thinsp; </p>
    <li><b>2024:</b> <a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#000000">ACM MM 2024 Outstanding Reviewer Award</a> (139 nominated). </li>
    <p style="line-height: 0.1;">&thinsp; </p>
    <li><b>2024:</b> <a href="https://bmvc2024.org/people/reviewers/" target="_blank" style="color:#000000">BMVC 2024 Outstanding Reviewer Award</a> (19.3%, 166/860). </li>
    <p style="line-height: 0.1;">&thinsp; </p>
    <li><b>2022:</b> Westlake University Summer Studentship (2/100+ selected for Stan Z. Li's Lab). </li>
</ul>



<!-- 
<p><b>Invited Talk</b>: </p>
<font size="3"> 
<ul>
<li>2023/12/14: Talk on "Mixup Data Augmentation for Computer Vision" @ Chongqing Technology and Business University</li> [<a href="./Files/Mixup_Data_Augmentation_for_Computer_Vision_20231214.pdf" target="_blank" style="color:#2a7ce0">PPT</a>]
<li>2023/12/14: Talk on "Introduction to AI Research and Experience Sharing" @ Chongqing Technology and Business University</li>
</ul>
</font>
<br />
-->

<A NAME="Acknowledgement"><h2>Acknowledgement</h2></A>
My research career cannot be possible without the support from all my awesome mentors, collaborators, and friends:
<ul>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>, Prof. <a href="https://yuzhou.vlrlab.net" target="_blank" style="color:#002FA7">Yu Zhou</a> at HUST.</li>
<li>Chair Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Stan Z. Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#002FA7">Zicheng Liu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=o5A23qIAAAAJ" target="_blank" style="color:#002FA7">Haitao Lin</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=egz8bGQAAAAJ" target="_blank" style="color:#002FA7">Jiangbin Zheng</a>, Mr. Siqi Ma at Westlake University.</li>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=ennCRJAAAAAJ" target="_blank" style="color:#002FA7">Zhenxing Mi</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=4OuUHYQAAAAJ" target="_blank" style="color:#002FA7">Yuxin Wang</a>, Mr. <a href="https://scholar.google.com/citations?hl=en&user=kR5LuzgAAAAJ" target="_blank" style="color:#002FA7">Yu Cai</a>, Mr. <a href="https://scholar.google.com/citations?hl=en&user=1xsk9rYAAAAJ" target="_blank" style="color:#002FA7">Yiwei Chen</a> at HKUST.</li>
<li>Mr. <a href="https://scholar.google.com/citations?hl=en&user=Ww37Oi4AAAAJ" target="_blank" style="color:#002FA7">Aozhong Zhang</a> at SUNY Albany.</li>
<li>Mr. <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#002FA7">Juanxi Tian</a> at HKBU.</li>
</ul>
<div style="text-align:justify">
<p style="line-height: 0.1;">&nbsp; </p>
Beyond academia, I feel incredibly fortunate to met wonderful friends and partners along the way (particularly during my middle school and high school years in Shenzhen). To every one of you who has walked through different chapters of my life: whether we are still in touch or not, I appreciate your presence and support, and carry deep gratitude for all our shared moments. I wish you all the best!
<p style="line-height: 0.1;">&nbsp; </p>
Even if most of you may never visit this page, please know my door is always open for a chat - coffee's on me!
</div>
<br />



<A NAME="Fragments of Memories"><h2>Fragments of Memories</h2></A>
Great memories with my advisors: Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a> (HUST). 
</p>
<!--
<a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Chair Prof. Stan Z. Li</a> (Westlake University), and also CAIRI AI Lab.
</p>
<img src="./Files/Zedong_With_Prof_Stan.jpeg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="340px" />&nbsp;</td>
<img src="./Files/CAIRI_WODeco.jpg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="382px" />&nbsp;</td>
-->
<img src="./Files/Zedong_With_Prof_Xinggang.jpeg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="190px" />&nbsp;</td>


<br />
<!-- 
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/889375a9-27d8-4975-bfa1-05b36916e021" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="352px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/d4a10ac1-2dbc-4e41-b286-4c1ffbd2954d" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="349px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/e55bbe34-3513-4cf0-8f6b-ef4c93f155ff" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="428px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/bd91d46c-0c14-484d-ac46-650f2c682e23" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="493px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/a414637c-a728-4695-b26c-2b00053d392a" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="460px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/0b34630b-5231-4742-8896-7e5851445cca" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="460px" />&nbsp;</td>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=n&d=p1SE8mvpEDXWAl_SKZ3NCPEvAjvDm5pAjUQB-nDQa5U&co=203b62&cmo=ebb653&cmn=ff5353&ct=ffffff' async="async"></script>
-->

<A NAME=""><h2></h2></A>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=305&t=n&d=p1SE8mvpEDXWAl_SKZ3NCPEvAjvDm5pAjUQB-nDQa5U&cmo=edbf4a&cmn=ff5f5f&co=306285'></script>

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {  
            $("#back_top").fadeIn(400); 
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script>




<!-- <font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2022.06.01</p>
</font> -->
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;All Rights Reserved by Zedong Wang. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.

</body>
</html>
