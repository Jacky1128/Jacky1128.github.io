<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Zedong Wang, çŽ‹æ³½æ ‹, Deep learning, HKUST">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/HKUST.ico">
<title>Zedong Wang</title>
<script async defer src="https://buttons.github.io/buttons.js"></script>
</head>


<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./" style="color:#1240A0"><img src="./Files/Profile_Twitter.jpeg" alt="" height="222px" /></a>&nbsp;</td>
<td align="left"><p><a href="./" style="color:#1240A0"><font size="5">Zedong Wang </font><font size="5"; font style="font-family:Microsoft YaHei">(Jacky)</font><font size="5"></font></a><br />
<i> <a href="https://cse.hkust.edu.hk" target="_blank" style="color:#1240A0">Department of Computer Science and Engineering (CSE)</a> </i>
<br />
<i> <a href="https://hkust.edu.hk" target="_blank" style="color:#1240A0">The Hong Kong University of Science and Technology (HKUST)</a> </i>

<!--
<br /><br />
<a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Dan Xu's Vision Group@HKUST</a><br />-->
<br /><br />
Location: Academic Building, HKUST, Clear Water Bay, Kowloon, Hong Kong.<br />

<!--
Location: E2-223, Westlake University, Dunyu Road #600, Xihu District, Hangzhou, Zhejiang, China.<br /> -->
<class="staffshortcut">
 <A HREF="#News" style="color:#1240A0">News</A> | 
 <A HREF="#Interests" style="color:#1240A0">Interests</A> | 
 <A HREF="#Education" style="color:#1240A0">Education</A> | 
 <A HREF="#Internship" style="color:#1240A0">Internship</A> | 
 <A HREF="#Publications" style="color:#1240A0">Publications</A> | 
 <A HREF="#Services" style="color:#1240A0">Services</A> | 
 <A HREF="#Awards" style="color:#1240A0">Awards</A> | 
 <A HREF="#Acknowledgement" style="color:#1240A0">Acknowledgement</A> | 
 <A HREF="#Fragments of Memories" style="color:#1240A0">Memories</A>  
 <!--<A HREF="./Files/ZedongWang_CV.pdf" style="color:#1240A0">CV</A> | 
 <A HREF="#Awards" style="color:#2a7ce0">Awards</A> -->
<br />
<br />

Email: zedong.wang@connect.ust.hk (prior); jackywang28@outlook.com; wangzedong@westlake.edu.cn;<br />
[<a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#1240A0">Google Scholar</a>]  
[<a href="https://www.semanticscholar.org/author/Zedong-Wang/2184760529" target="_blank" style="color:#1240A0">Semantic Scholar</a>] 
[<a href="https://openreview.net/profile?id=~Zedong_Wang1" target="_blank" style="color:#1240A0">OpenReview</a>] 
[<a href="https://twitter.com/ZedongWangAI" target="_blank" style="color:#1240A0">Twitter (X)</a>]
[<a href="https://huggingface.co/ZedongWangAI" target="_blank" style="color:#1240A0">ðŸ¤—HF</a>]
[<a href="https://github.com/Jacky1128" target="_blank" style="color:#1240A0">GitHub</a>] 
[<a href="https://orcid.org/0009-0000-0112-0491" target="_blank" style="color:#1240A0">ORCID</a>] 
[<a href="https://dblp.org/pid/179/8811.html" target="_blank" style="color:#1240A0">DBLP</a>] 

<br />
<br />
<i>I am looking for long-term research collaboration on related topics, including Efficient Neural Network Architectures, Efficient Multi-task and Multi-modal Learning. Feel free to contact me!</i><br />
</td></tr></table>


<A NAME="Short Bio"><h2>Short Bio</h2></A>
<div style="text-align:justify; line-height:126%">
Zedong Wang (çŽ‹æ¾¤æ£Ÿ) is currently a first-year PhD student in Computer Science and Engineering at <a href="https://hkust.edu.hk" target="_blank" style="color:#1240A0">The Hong Kong University of Science and Technology (HKUST)</a>, 
advised by <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Prof. Dan Xu</a>. Previously, he was a visiting student at <a href="https://en.westlake.edu.cn" target="_blank" style="color:#1240A0">Westlake University</a>, 
advised by <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Chair Prof. Stan Z. Li (IEEE Fellow, IAPR Fellow)</a>.
He received his bachelor's degree in Electronics and Information Engineering from <a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#1240A0">Huazhong University of Science and Technology (HUST)</a> in 2023, where he was fortunately advised by <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a>.
</p>
He has been awarded as Outstanding Reviewer by several conferences such as ECCV, ACM MM, and BMVC. His research interests center on computer vision and multi-task learning, with a special focus on efficient network architectures and optimization techniques, as well as their applications in autonomous driving.
</div>


<A NAME="News"><h2>News</h2></A>
  <!-- <div style="height:200px;overflow-y:auto;background:#ffffff;"> -->
  <div style="height:250px;overflow-y:auto;">
  <ul>
    <li><b> <font color="#9F7320">[2024.12.12]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://icml.cc/Conferences/2025" target="_blank" style="color:#000000">ICML 2025</a></b>, which will be held in Vancouver, Canada.</li></li>
    <li><b> <font color="#9F7320">[2024.11.04]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> for <b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC 2024 (<font color="#FF0000">rate: 19.3%, 166/860</font>)</a></b>. Happy to contribute to the community! Huge thanks to the BMVC organizers for their dedication.</li></li>
    <li><b> <font color="#9F7320">[2024.11.04]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> for <b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM 2024 (<font color="#FF0000">rate: 139/X</font>)</a></b>. Happy to contribute to the community! Huge thanks to the ACM MM organizers for their dedication.</li></li>
    <li><b> <font color="#9F7320">[2024.11.04]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://cvpr.thecvf.com/Conferences/2025" target="_blank" style="color:#000000">CVPR 2025</a></b>, which will be held in Nashville TN, USA.</li></li>
    <li><b> <font color="#9F7320">[2024.10.10]</font> </b> One paper on <i>vision backbones & optimizers</i>, <i><a href="https://arxiv.org/abs/2410.06373" target="_blank" style="color:#0e19ee"> BOCB</a></i>, is released. Welcome to visit the <a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#1240A0">HF</a> and <a href="https://bocb-ai.github.io" target="_blank" style="color:#1240A0">Project</a> page for further discussion! Happy to explore something interesting together with <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#1240A0">Juanxi Tian</a> and <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#1240A0">Siyuan Li</a>!</li></li>
    <li><b> <font color="#9F7320">[2024.09.24]</font> </b> I am recognized as an <i>Outstanding Reviewer</i> for <b><a href="https://eccv.ecva.net/Conferences/2024" target="_blank" style="color:#000000">ECCV 2024 (<font color="#FF0000">rate: 2.7%, 198/7293</font>)</a></b>. Happy to contribute to the community! Huge thanks to the ECCV organizers for their dedication.</li></li>
    <li><b> <font color="#9F7320">[2024.08.15]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://iclr.cc/Conferences/2025" target="_blank" style="color:#000000">ICLR 2025</a></b>, which will be held in Singapore.</li></li>
    <li><b> <font color="#9F7320">[2024.07.20]</font> </b> I am invited to serve as <i>Program Committee Member</i> for <b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI 2025</a></b>, which will be held in Philadelphia, USA. </li></li>
    <li><b> <font color="#9F7320">[2024.07.14]</font> </b> Maintain an open-source repository on <i>optimizers</i>, <i><a href="https://github.com/tianshijing/Awesome-Optimizers" target="_blank" style="color:#0e19ee">Awesome-Optimizers</a></i>, with <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#1240A0">Juanxi Tian</a> and <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#1240A0">Siyuan Li</a>. </li>
    <li><b> <font color="#9F7320">[2024.06.04]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://neurips.cc/Conferences/2024/CallForDatasetsBenchmarks" target="_blank" style="color:#000000">NeurIPS 2024 (Datasets and Benchmarks Track)</a></b>. </li></li>
    <li><b> <font color="#9F7320">[2024.05.02]</font> </b> One paper on <i>AI4Science (genomics)</i>, <i><a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#0e19ee"> VQDNA</a></i>, is accepted at <b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML 2024</a></b>. </li></li>
    <li><b> <font color="#9F7320">[2024.05.02]</font> </b> Co-authored paper on <i>long sequence modeling</i>, <i><a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#0e19ee"> CHELA</a></i>, is accepted at <b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#1240A0">Zicheng Liu</a>! </li></li>
    <li><b> <font color="#9F7320">[2024.05.01]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC 2024</a></b>, which will be held in Glasgow, UK.</li></li>
    <li><b> <font color="#9F7320">[2024.04.16]</font> </b> Co-authored paper on <i>long sequence modeling</i>, <i><a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#0e19ee"> LongVQ</a></i>, is accepted at <b><a href="https://ijcai24.org" target="_blank" style="color:#000000">IJCAI 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#1240A0">Zicheng Liu</a>! </li></li>
    <li><b> <font color="#9F7320">[2024.04.13]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM 2024</a></b>, which will be held in Melbourne, Australia. </li></li>
    <li><b> <font color="#9F7320">[2024.03.12]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://eccv.ecva.net/Conferences/2024" target="_blank" style="color:#000000">ECCV 2024</a></b>, which will be held in MiCo Milano, Italy.</li></li>
    <li><b> <font color="#9F7320">[2024.01.22]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://icml.cc/Conferences/2024" target="_blank" style="color:#000000">ICML 2024</a></b>, which will be held in Vienna, Austria.</li></li>
    <li><b> <font color="#9F7320">[2024.01.17]</font> </b> I am invited to serve as a <i>Reviewer</i> for <b><a href="https://icpr2024.org" target="_blank" style="color:#000000">ICPR 2024</a></b>, which will be held in Kolkata, India.</li></li>
    <li><b> <font color="#9F7320">[2024.01.16]</font> </b> One paper on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#0e19ee"> MogaNet</a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#1240A0">Code & weights</a> (180 stars) are released!</li></li>
    <li><b> <font color="#9F7320">[2024.01.16]</font> </b> Co-authored paper on <i>semi-sup learning</i>, <i><a href="https://arxiv.org/abs/2310.03013" target="_blank" style="color:#0e19ee"> SemiReward</a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#1240A0">Siyuan Li</a>! </li></li>
    <li><b> <font color="#9F7320">[2024.01.09]</font> </b> I am invited to serve as an <i> emergency reviewer</i> for <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b> <b><a href="https://iclr.cc/Conferences/2024/CallForTinyPapers" target="_blank" style="color:#000000">(TinyPapers)</a></b>. It will be held in Vienna, Austria. </li></li>
    <li><b> <font color="#9F7320">[2023.12.31]</font> </b> Co-authored preprint on <i>self-supervised learning</i>, <i><a href="https://arxiv.org/abs/2401.00897" target="_blank" style="color:#0e19ee"> Masked Modeling on Vision and Beyond</a></i>. </li></li>
    <li><b> <font color="#9F7320">[2023.09.31]</font> </b> Co-authored paper on <i>video prediction</i>, <i><a href="https://arxiv.org/abs/2306.11249" target="_blank" style="color:#0e19ee"> OpenSTL</a></i>, is accepted at <b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">NeurIPS 2023</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=6kTV6aMAAAAJ" target="_blank" style="color:#1240A0">Cheng Tan</a>! </li></li>
    <li><b> <font color="#9F7320">[2023.06.25]</font> </b> Got my B.Eng. degree from Huazhong University of Science and Technology (HUST)! Special thanks to my undergraduate supervisor <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a> for the generous support!</li>
    <li><b> <font color="#9F7320">[2023.05.23]</font> </b> One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2111.15454" target="_blank" style="color:#0e19ee"> SAMix</a></i>, is presented for both SL & SSL scenarios. </li>
    <li><b> <font color="#9F7320">[2022.11.07]</font> </b> One preprint on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#0e19ee"> MogaNet</a></i>. A new family of pure convolutional architecture covering <b>5M~100M+</b> model scales with great performance.  <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#1240A0">Code & weights</a> are released (180 stars). Welcome to discuss, use, and star!</li>
    <li><b> <font color="#9F7320">[2022.09.11]</font> </b> One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2209.04851" target="_blank" style="color:#0e19ee">OpenMixup</a></i>, is presented for vision tasks. This is also my first arXiv paper!</li>
    <li><b> <font color="#9F7320">[2022.09.11]</font> </b> Maintain an open-source repository, <i><a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#0e19ee">OpenMixup</a></i> (618 stars), for both supervised, semi- and self-supervised visual representation learning based on PyTorch. On updating! </li>
    <li><b> <font color="#9F7320">[2022.07.06]</font> </b> Fortunate to become <i>visiting student</i> at Westlake University, under the supervision of <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Chair Prof. Stan Z. Li</a>. </li></li>
    <li><b> <font color="#9F7320">[2021.09.01]</font> </b> Fortunate to become <i>research intern</i> in <a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#1240A0"> HUST Vision Lab</a>, under the supervision of <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a>. </li></li>
    <li><b> <font color="#9F7320">[2021.06.01]</font> </b> Fortunate to become <i>research intern</i> in <a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#1240A0"> SIAT-MMLab</a> at Shenzhen Institute of Advanced Technology, CAS. </li></li>

</ul>
</div>


<A NAME="Interests"><h2>Research Interests</h2></A>
Currently, I focus mostly on Efficient Multi-task Representation Learning, including (but not limited to):
<ul>
    <li>Data-Efficient Learning: Mixup Augmentation [<a href="https://arxiv.org/abs/2209.04851" target="_blank" style="color:#1240A0">OpenMixup</a>, <a href="https://arxiv.org/abs/2111.15454" target="_blank" style="color:#1240A0">SAMix</a>], Semi-supervised Learning [<a href="https://arxiv.org/abs/2310.03013" target="_blank" style="color:#1240A0">SemiReward</a>].</li>
    <li>Efficient Network Architectures: Vision Backbones [<a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#1240A0">MogaNet</a>], Multi-task Architectures.</li>
    <li>Optimization Techniques: Optimizers [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#1240A0">SEMA</a>, <a target="_blank" style="color:#1240A0">BOCB</a>], Multi-task Optimization Methods.</li>
    <li>Downstream Applications: End-to-End Autonomous Driving.</li>
</ul>


<A NAME="Education"><h2>Education</h2></A>

<p style="line-height: 0.2;">&nbsp; </p>

<p style="margin-left: 1.9em;"> 
    <img src="Files/HKUST.ico" align="left" width="55" height="73"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://hkust.edu.hk" target="_blank" style="color:#000000">The Hong Kong University of Science and Technology</a> (2025-now) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; Ph.D. in Computer Science and Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Efficient Multi-task Learning. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Supervisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Prof. <b>Dan Xu</b></a>.<br>
</p>
<p style="line-height: 0.4;">&nbsp; </p>

<p style="margin-left: 1.0em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="67"/>
    &nbsp&nbsp&nbsp&nbsp <b><a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#000000">Huazhong University of Science and Technology</a> (2019-2023) </b><br> 
    &nbsp&nbsp&nbsp&nbsp &bull; B.Eng. in Electronic and Information Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Thesis:</i> Efficient ConvNet-based Vision Backbone for Multiple Tasks (Grade: 92/100). <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Supervisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. <b>Xinggang Wang</b></a>.<br>
</p>
<p style="line-height: 0.1;">&nbsp; </p>

<A NAME="Internship"><h2>Research Experience</h2></A>
<!--
<ul>
<li>2024.04-now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on multi-task perception at <a href="https://en.wikipedia.org/wiki/Zeekr" target="_blank" style="color:#1240A0">ZEEKR, Geely Holding Group</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Prof. Dan Xu</a>.</li>
<li>2024.03-now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp;&thinsp; Research on multi-modal & multi-task learning at <a href="https://hkust.edu.hk" target="_blank" style="color:#1240A0">HKUST</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp;&thinsp;&nbsp;&nbsp;&thinsp;&thinsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Prof. Dan Xu</a>.</li>
<li>2022.06-now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp;&thinsp; Research on representation learning & AI4S at <a href="https://en.westlake.edu.cn" target="_blank" style="color:#1240A0">Westlake University</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Chair Prof. Stan Z. Li</a>.</li>
<li>2021.09-2023.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on few-shot semantic segmentation at <a href="https://www.hust.edu.cn" target="_blank" style="color:#1240A0">HUST Vision Lab</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a>.</li>
<li>2021.06-2021.09 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp; Research on semantic segmentation & text spotting at <a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#1240A0">SIAT-MMLab</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;&thinsp;&thinsp;&thinsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#1240A0">Dr. Bin Fu</a>.</li>
<li>2020.06-2021.04 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on remote sensing high-res semantic segmentation at <a href="http://www.digitalearthlab.com.cn" target="_blank" style="color:#1240A0">CAS</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Advisor: <a href="https://scholar.google.com/citations?hl=en&user=TpX2C3cAAAAJ" target="_blank" style="color:#1240A0">Dr. Xiaoping Du</a>.</li>
</ul>
<br /> 
-->
<p style="line-height: 0.3;">&nbsp; </p>

<p style="margin-left: 1.7em;"> 
    <img src="Files/ZEEKR_logo.png" align="left" width="65" height="65"/>
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://en.wikipedia.org/wiki/Zeekr" target="_blank" style="color:#000000">ZEEKR Intelligent Technology</a> (2024.04-present) </b><br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Efficient Multi-task Learning in Autonomous Driving. <br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Prof. <b>Dan Xu</b></a> (University-Enterprise Cooperation). <br>
</p>

<p style="line-height: 0.6;">&nbsp; </p>


<p style="margin-left: 1.9em;"> 
    <img src="Files/westlake.ico" align="left" width="62" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#000000">Stan Z. Li's AI Lab, Westlake University</a> (2022.06-present) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Visual Representation Learning and AI for Genomics. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Chair Prof. <b>Stan Z. Li</b> (IEEE Fellow, IAPR Fellow)</a>.<br>
</p>

<p style="line-height: 0.6;">&nbsp; </p>

<p style="margin-left: 1.2em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="65"/>
    &nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#000000">Xinggang Wang's Vision Group, HUST</a> (2021.09-2022.06) </b><br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Efficient Visual Recognition and Few-shot Semantic Segmentation. <br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. <b>Xinggang Wang</b></a>.<br>
</p>

<p style="line-height: 0.6;">&nbsp; </p>

<p style="margin-left: 1.9em;"> 
    <img src="Files/SIAT_logo.png" align="left" width="64" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; <b><a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#000000">SIAT-MMLab, Chinese Academy of Sciences</a> (2021.06-2021.09) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Topics:</i> Semantic Segmentation and Text Spotting. <br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Advisor:</i> <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#1240A0">Dr. <b>Bin Fu</b></a>.<br>
</p>

<p style="line-height: 0.3;">&nbsp; </p>

<A NAME="Publications"><h2>Publications</h2></A>

<p><b>Selected Preprints (*: Equal Contribution. â€ : Corresponding Author.)</b>: </p>
<font size="3"> 
<ul>
<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_BOCB.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a target="_blank" style="color:#FF0000">[NEW!]</a></b> <b><a href= "https://huggingface.co/papers/2410.06373" target="_blank" style="color:#1240A0">Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning</a></b></font><br>
        <i> Siyuan Li*, Juanxi Tian*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Luyuan Zhang, Zicheng Liu, Weiyang Jin, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2410.06373" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#1240A0">ðŸ¤—HF</a>]
        [<a href="https://bocb-ai.github.io" target="_blank" style="color:#1240A0">Project</a>]
        [<a href="https://github.com/Black-Box-Optimization-Coupling-Bias/BOCB" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_SEMA.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://huggingface.co/papers/2402.09240" target="_blank" style="color:#1240A0">Switch EMA: A Free Lunch for Better Flatness and Sharpness</a></b></font><br>
        <i> Siyuan Li*, Zicheng Liu*, Juanxi Tian*, Ge Wang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://huggingface.co/papers/2402.09240" target="_blank" style="color:#1240A0">ðŸ¤—HF</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2023_MM_Survey.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#1240A0">Masked Modeling for Self-supervised Representation Learning on Vision and Beyond</a></b></font><br>
        <i> Siyuan Li*, Luyuan Zhang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Di Wu, Lirong Wu, Zicheng Liu, Jun Xia, Cheng Tan, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2023</b></i><br>
        [<a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Lupin1998/Awesome-MIM" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/arXiv_2023_MIMSurvey_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2022_OpenMixup.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2209.04851" target="_blank" style="color:#1240A0">OpenMixup: A Comprehensive Mixup Benchmark for Visual Classification</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Zicheng Liu, Di Wu, Cheng Tan, Weiyang Jin, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2022</b></i><br>
        [<a href= "https://arxiv.org/abs/2209.04851" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/arxiv_2022_OpenMixup_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv22_SAMix.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2111.15454" target="_blank" style="color:#1240A0">Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup</a></b></font><br>
        <i> Siyuan Li*, Zicheng Liu*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Di Wu, Zihan Liu, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2022</b></i><br>
        [<a href= "https://arxiv.org/abs/2111.15454" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/arXiv_2021_SAMix_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>


</ul>
<br />


<p><b>Conferences (As First Author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_VQDNA.jpeg" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#1240A0">VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Zicheng Liu, Di Wu, Cheng Tan, Jiangbin Zheng, Yufei Huang, Stan Z. Liâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Lupin1998/VQDNA" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/icml_2024_VQDNA_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_MogaNet.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2211.03295" target="_blank" style="color:#1240A0">MogaNet: Multi-order Gated Aggregation Network</a></b></font><br>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang*</a></b>, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Liâ€  </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2211.03295" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#1240A0">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/582542948" target="_blank" style="color:#1240A0">Zhihu</a>]
        [<a href="./Files/iclr_2024_MogaNet_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>

</ul>

<p><b>Conferences (As Co-author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_CHELA.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#1240A0">Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences</a></b></font><br>
        <i> Zicheng Liu, Siyuan Li, Li Wang, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Yunfan Liu, Stan Z. Liâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/icml_2024_CHELA_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/IJCAI24_LongVQ.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#1240A0">LongVQ: Long Sequence Modeling with Vector Quantization on Structured Memory</a></b></font><br>
        <i> Zicheng Liu, Li Wang, Siyuan Li, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Haitao Lin, Stan Z. Liâ€  </a></i><br><i><b><a href="https://ijcai24.org" target="_blank" style="color:#000000">International Joint Conference on Artificial Intelligence (IJCAI), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/ijcai_2024_LongVQ_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>
    

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_SemiReward.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2310.03013" target="_blank" style="color:#1240A0">SemiReward: A General Reward Model for Semi-supervised Learning</a></b></font><br>
        <i> Siyuan Li*, Weiyang Jin*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Liâ€  </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2310.03013" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/SemiReward" target="_blank" style="color:#1240A0">Code</a>]
        [<a href="./Files/iclr_2024_SemiReward_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/NeurIPS23_OpenSTL.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2306.11249" target="_blank" style="color:#1240A0">OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning</a></b></font><br>
        <i> Cheng Tan*, Siyuan Li*, Zhangyang Gao, Wenfei Guan, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#000000">Zedong Wang</a></b>, Zicheng Liu, Lirong Wu, Stan Z. Liâ€  </a></i><br><i><b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">Advances in Neural Information Processing Systems (NeurIPS), 2023</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2306.11249" target="_blank" style="color:#1240A0">arXiv</a>]
        [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#1240A0">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/640271275" target="_blank" style="color:#1240A0">Zhihu</a>]
        [<a href="./Files/NIPS_2023_OpenSTL_bibtex" target="_blank" style="color:#1240A0">BibTeX</a>]
</p></td></tr></table>   


</ul>

<!--
<p><b>Journals</b>: </p>
-->
<font size="3"> 
<ul>

</ul>


<A NAME="Services"><h2>Professional Services</h2></A>

<p><b>Program Committee Member / Reviewer</b>: </p>
<font size="3"> 
<ul>
<!--
<li><b>Conference Reviewer / PC Member:</b></li>-->
<li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b><a href="https://cvpr.thecvf.com" target="_blank" style="color:#000000">CVPR</a></b>), 2025<br /></li>
<li>International Conference on Learning Representations (<b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR</a></b>), 2024 (TinyPapers), 2025<br /></li>
<li>International Conference on Machine Learning (<b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML</a></b>), 2024, 2025<br /></li>
<li>European Conference on Computer Vision (<b><a href="https://eccv.ecva.net" target="_blank" style="color:#000000">ECCV</a></b>), 2024<br /></li>
<li>Conference on Neural Information Processing Systems (<b><a href="https://neurips.cc" target="_blank" style="color:#000000">NeurIPS</a></b>), 2024 (D&B Track)<br /></li>
<li>AAAI Conference on Artificial Intelligence (<b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI</a></b>), 2025<br /></li>
<li>ACM International Conference on Multimedia (<b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM</a></b>), 2024<br /></li>
<li>BMVA The British Machine Vision Conference (<b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC</a></b>), 2024<br /></li>
<li>IAPR International Conference on Pattern Recognition (<b><a href="https://icpr2024.org" target="_blank" style="color:#000000">ICPR</a></b>), 2024<br /></li>
<!--
<li><b>Journal Reviewer:</b></li>
-->
</ul>
</font>
<br />
 
<p><b>Membership</b>: </p>
<font size="3"> 
<ul>
<li>China Computer Federation (<b><a href="https://www.ccf.org.cn/en/" target="_blank" style="color:#000000">CCF</a></b>), Student Member, 2024-2026</li>
<li>China Society of Image and Graphics (<b><a href="https://en.csig.org.cn" target="_blank" style="color:#000000">CSIG</a></b>), Student Member, 2023</li>
</ul>
</font>
<br />


<A NAME="Awards"><h2>Selected Awards and Scholarships</h2></A>
<p style="line-height: 0.1;">&nbsp; </p>

<ul>
    <li><b>2024:</b> HKUST Postgraduate Studentship (HK$18,760/month)</li>
    <p style="line-height: 0.1;">&thinsp; </p>
    <li><b>2024:</b> <a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#000000">ECCV 2024 Outstanding Reviewer Award (rate: 2.7%, 198/7293)</a>. </li>
    <p style="line-height: 0.1;">&thinsp; </p>
    <li><b>2024:</b> <a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#000000">ACM MM 2024 Outstanding Reviewer Award (rate: 139/X)</a>. </li>
    <p style="line-height: 0.1;">&thinsp; </p>
    <li><b>2024:</b> <a href="https://bmvc2024.org/people/reviewers/" target="_blank" style="color:#000000">BMVC 2024 Outstanding Reviewer Award (rate: 19.3%, 166/860)</a>. </li>
    <p style="line-height: 0.1;">&thinsp; </p>
    <li><b>2022:</b> Westlake University Summer Studentship (2/100+ selected for Stan Z. Li's Lab). </li>
</ul>



<!-- 
<p><b>Invited Talk</b>: </p>
<font size="3"> 
<ul>
<li>2023/12/14: Talk on "Mixup Data Augmentation for Computer Vision" @ Chongqing Technology and Business University</li> [<a href="./Files/Mixup_Data_Augmentation_for_Computer_Vision_20231214.pdf" target="_blank" style="color:#2a7ce0">PPT</a>]
<li>2023/12/14: Talk on "Introduction to AI Research and Experience Sharing" @ Chongqing Technology and Business University</li>
</ul>
</font>
<br />
-->

<A NAME="Acknowledgement"><h2>Acknowledgement</h2></A>
My research career cannot be possible without the generous support from all my awesome mentors, collaborators, and friends:
<ul>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Xinggang Wang</a>, Prof. <a href="https://yuzhou.vlrlab.net" target="_blank" style="color:#1240A0">Yu Zhou</a> at Huazhong University of Science and Technology (HUST).</li>
<li>Chair Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Stan Z. Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#1240A0">Siyuan Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#1240A0">Zicheng Liu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=o5A23qIAAAAJ" target="_blank" style="color:#1240A0">Haitao Lin</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=egz8bGQAAAAJ" target="_blank" style="color:#1240A0">Jiangbin Zheng</a>, Mr. Siqi Ma at Westlake University.</li>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#1240A0">Dan Xu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=ennCRJAAAAAJ" target="_blank" style="color:#1240A0">Zhenxing Mi</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=4OuUHYQAAAAJ" target="_blank" style="color:#1240A0">Yuxin Wang</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=kR5LuzgAAAAJ" target="_blank" style="color:#1240A0">Yu Cai</a>, Mr. <a href="https://scholar.google.com/citations?hl=en&user=1xsk9rYAAAAJ" target="_blank" style="color:#1240A0">Yiwei Chen</a> at The Hong Kong University of Science & Technology.</li>
<li>Dr. <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#1240A0">Bin Fu</a>, Mr. <a href="https://scholar.google.com/citations?hl=en&user=Ww37Oi4AAAAJ" target="_blank" style="color:#1240A0">Aozhong Zhang</a> at SIAT-MMLab, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences.</li>
<li>Dr. <a href="https://scholar.google.com/citations?hl=en&user=TpX2C3cAAAAJ" target="_blank" style="color:#1240A0">Xiaoping Du</a> at Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences.</li>
<li>Mr. <a href="https://scholar.google.com/citations?hl=en&user=6QB6Q8gAAAAJ" target="_blank" style="color:#1240A0">Juanxi Tian</a> at Hong Kong Baptist University.</li>
</ul>
<div style="text-align:justify">
Apart from research, I've been fortunate to meet wonderful friends and partners at different life stages. Whether we are still in touch or not, I appreciate all your presence and support and wish you all the best. To the very best of times!
</div>
<br />



<A NAME="Fragments of Memories"><h2>Fragments of Memories</h2></A>
Great memories with my advisors: <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#1240A0">Prof. Xinggang Wang</a> (HUST). 
</p>
<!--
<a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#1240A0">Chair Prof. Stan Z. Li</a> (Westlake University), and also CAIRI AI Lab.
</p>
<img src="./Files/Zedong_With_Prof_Stan.jpeg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="340px" />&nbsp;</td>
<img src="./Files/CAIRI_WODeco.jpg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="382px" />&nbsp;</td>
-->
<img src="./Files/Zedong_With_Prof_Xinggang.jpeg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="190px" />&nbsp;</td>


<br />
<!-- 
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/889375a9-27d8-4975-bfa1-05b36916e021" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="352px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/d4a10ac1-2dbc-4e41-b286-4c1ffbd2954d" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="349px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/e55bbe34-3513-4cf0-8f6b-ef4c93f155ff" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="428px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/bd91d46c-0c14-484d-ac46-650f2c682e23" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="493px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/a414637c-a728-4695-b26c-2b00053d392a" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="460px" />&nbsp;</td>
<img src="https://github.com/Westlake-AI/openmixup/assets/55654777/0b34630b-5231-4742-8896-7e5851445cca" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="460px" />&nbsp;</td>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=n&d=p1SE8mvpEDXWAl_SKZ3NCPEvAjvDm5pAjUQB-nDQa5U&co=203b62&cmo=ebb653&cmn=ff5353&ct=ffffff' async="async"></script>
-->

<A NAME=""><h2></h2></A>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=305&t=n&d=p1SE8mvpEDXWAl_SKZ3NCPEvAjvDm5pAjUQB-nDQa5U&cmo=edbf4a&cmn=ff5f5f&co=306285'></script>

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {  
            $("#back_top").fadeIn(400); 
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script>




<!-- <font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2022.06.01</p>
</font> -->
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;All Rights Reserved by Zedong Wang. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.

</body>
</html>
