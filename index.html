<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Zedong Wang, Deep learning, Computer Vision, HKUST">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/HKUST_logo.png">
<title>Zedong Wang</title>
<script async defer src="https://buttons.github.io/buttons.js"></script>

<style>
  .profile-table {
    margin-bottom: 25px;
    border: none;
    margin-top: 2px;
    margin-left: 2px;
  }
  .profile-table td {
    text-align: left;
    border: none;
    padding: 10px;
    vertical-align: top;
    font-size: 1.03em;
  }

  .dashed-underline-me {
    border-bottom: 1px dashed;
    text-decoration: none;
    color: #000000;
  }

  /* News Section Styling */
  .news-list {
    list-style: none;
    padding-left: 0;
  }

  .news-item {
    background-color: #ffffff;
    border-left: 2px solid #9F6821;
    padding: 8px 15px;
    margin-bottom: -4px;
    border-radius: 5px;
    transition: all 0.3s ease;
    box-shadow: 0 2px 5px rgba(0,0,0,0.05);
  }

  .news-item:hover {
    transform: translateY(0px);
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
  }

  .news-date {
    font-weight: bold;
    color: #9F6821;
    margin-right: 10px;
  }

  /* Awards Section Redesign */
  .awards-grid {
      list-style: none;
      padding: 0;
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
      gap: 15px;
      margin-top: 5px;
  }

  .award-card {
      background-color: #fdfdff;
      border-radius: 8px;
      padding: 20px;
      text-align: center;
      border: 1px solid #e8e8e8;
      transition: all 0.3s ease;
      box-shadow: 0 4px 6px rgba(0,0,0,0.04);
      display: flex;
      flex-direction: column;
      justify-content: space-between;
  }

  .award-card:hover {
      transform: translateY(-4px);
      box-shadow: 0 8px 25px rgba(0,0,0,0.08);
      border-color: #002FA7;
  }

  .award-card .icon {
      font-size: 2.2em;
      color: #002FA7;
      margin-bottom: 12px;
  }

  .award-card .conference {
      font-weight: bold;
      font-size: 1.1em;
      margin-bottom: 8px;
  }

  .award-card .details {
      font-size: 0.95em;
      color: #555;
      line-height: 1.4;
  }

  .award-card .details font {
      font-weight: bold;
  }
</style>

</head>


<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 


<table class="profile-table"><tr><td>
<a href="./" style="color:#002FA7"><img src="./Files/Profile_Home.jpg" alt="" height="214px" /></a>&nbsp;</td>
<td align="left"><p><a href="./" style="color:#000000"><font size="5">WANG Ze Dong (çŽ‹æ³½æ ‹)</font></a><br />
<i><a href="https://cse.hkust.edu.hk/admin/factsheet/" target="_blank" style="color:#002FA7">Department of Computer Science and Engineering (CSE)</a> </i>
<br />
<i><a href="https://hkust.edu.hk" target="_blank" style="color:#002FA7">The Hong Kong University of Science and Technology (HKUST)</a> </i>

<br />
<div style="display:flex; flex-direction:column; gap:3px;">
    <div>Address: RPg Hub 1007, Academic Building, HKUST, Clear Water Bay, Kowloon, Hong Kong (<a href="https://www.google.com/maps/search/HKUST+map/@22.335677,114.2613186,17z" target="_blank" style="color:#002FA7">MAP</a>).</div>
    <div class="navigation-bar">
        <a href="#News" class="nav-link">News</a> | 
        <a href="#Interests" class="nav-link">Interests</a> | 
        <a href="#Education" class="nav-link">Education</a> | 
        <a href="#Internship" class="nav-link">Interns</a> | 
        <a href="#Awards" class="nav-link">Awards</a> | 
        <a href="#Publications" class="nav-link">Pubs</a> | 
        <a href="#Services" class="nav-link">Services</a> | 
        <a href="#Acknowledgement" class="nav-link">Acknowledgement</a> | 
        <a href="./Files/Jacky_CV.pdf" class="nav-link">ResumÃ© (CV)</a>
  </div></div>
  
<style>
  .navigation-bar {
    gap: 4px;
    margin: 0 0 15px 0; /* Removed top margin */
  }
  .nav-link {
    color: #002FA7;
    text-decoration: none;
    border-radius: 5px;
  }
  .nav-link:hover {
    background-color: #fafbff;
  }
</style>

<!-- Add in the head section -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

Email: zedong.wang@connect.ust.hk (preferred); zwangmw@cse.ust.hk; wangzedong@westlake.edu.cn;<br />
[<a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#002FA7">Google Scholar</a>]  
[<a href="https://openreview.net/profile?id=~Zedong_Wang1" target="_blank" style="color:#002FA7">OpenReview</a>]
[<a href="https://twitter.com/ZedongWangAI" target="_blank" style="color:#002FA7">Twitter (X)</a>]
[<a href="https://huggingface.co/ZedongWangAI" target="_blank" style="color:#002FA7">Hugging Face</a>] 
[<a href="https://github.com/Jacky1128" target="_blank" style="color:#002FA7">GitHub</a>] 
[<a href="https://orcid.org/0009-0000-0112-0491" target="_blank" style="color:#002FA7">ORCID</a>]  
[<a href="https://www.linkedin.com/in/jacky-zedong-wang/" target="_blank" style="color:#002FA7">LinkedIn</a>]  
[<a href="https://www.strava.com/athletes/104629081" target="_blank" style="color:#002FA7">Strava</a>]


<br />
<br />
<i><a href="https://x.com/fchollet/status/1894227128302145970" target="_blank" style="color:#000000">Humor, art, science, and being kind to each other is how you preserve your sanity in a darkening world.</a>
</i><br />
</td></tr></table>


<A NAME="Short Bio"><h2>Short Bio</h2></A>
<div style="text-align:justify; line-height:132%">
Zedong Wang is currently a <a href="https://cse.hkust.edu.hk/admin/factsheet/" target="_blank" style="color:#002FA7">CSE</a> PhD student at The Hong Kong University of Science and Technology (<a href="https://hkust.edu.hk" target="_blank" style="color:#002FA7">HKUST</a>), 
advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>. Previously, he was a visiting student at <a href="https://en.westlake.edu.cn" target="_blank" style="color:#002FA7">Westlake University</a>. He received his bachelor's degree in Electronics and Information Engineering from Huazhong University of Science and Technology (<a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#002FA7">HUST</a>), where he was fortunate to be advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.
</p>
His research centers on deep learning, computer vision, and multi-task learning, with a specific focus on (i) efficient model architectures, (ii) unified understanding & generation frameworks, and (iii) multi-task / (M)LLM optimizers. He has been recognized as Outstanding/Notable Reviewer at several prestigious conferences, including <a href="https://iclr.cc/Conferences/2025/Reviewers" target="_blank" style="color:#002FA7">ICLR 2025</a>, <a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#002FA7">ECCV 2024</a>, <a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#002FA7">ACM MM 2024</a>, and <a href="http://bmvc2024.org/people/reviewers/" target="_blank" style="color:#002FA7">BMVC 2024</a>.



<A NAME="News"><h2>News</h2></A>
  <div style="height:320px;overflow-y:auto;">
    <ul class="news-list">
      <li class="news-item"><span class="news-date">[2025.05.20]</span>I was listed as an <i>Notable Reviewer</i> at <b><a href="https://iclr.cc/Conferences/2025/Reviewers" target="_blank" style="color:#000000">ICLR 2025 <font color="#b80000">(rate: 2.6%, 473/18323)</font></a></b>. Happy to contribute to ICLR!</li>
      <li class="news-item"><span class="news-date">[2025.05.16]</span>One paper on <i>(M)LLM optimizers</i>, <i><a href="https://huggingface.co/papers/2506.01049" target="_blank" style="color:#002FA7"> <b>SGG</b></a></i>, is accepted at <b><a href="https://2025.aclweb.org" target="_blank" style="color:#000000">ACL 2025 main</a></b>. Congrats to all co-authors! </li>
      <li class="news-item"><span class="news-date">[2025.04.04]</span>One co-authored paper on <i>visual generation & representation learning</i>, <i><a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#002FA7"> <b>MergeVQ</b></a></i>, is accepted at <b><a href="https://cvpr.thecvf.com/Conferences/2025" target="_blank" style="color:#000000">CVPR 2025</a></b>, <a href="https://huggingface.co/papers/date/2025-04-03" target="_blank" style="color:#002FA7"><b>ranking 1st</b> in ðŸ¤—Hugging Face Daily Papers</a>, and <a href="https://huggingface.co/papers/week/2025-W14" target="_blank" style="color:#002FA7"><b>ranking 4th</b> in Weekly Papers.</a> Congrats to all co-authors! </li>
      <li class="news-item"><span class="news-date">[2024.12.12]</span>One co-authored paper on <i>low-level vision</i>, <i><a href="https://arxiv.org/abs/2503.01136" target="_blank" style="color:#002FA7"> <b>PGHHNet</b></a></i>, is accepted at <b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI 2025</a></b>. Congrats to all co-authors!</li>
      <li class="news-item"><span class="news-date">[2024.11.04]</span>I was listed as an <i>Outstanding Reviewer</i> at <b><a href="https://bmvc2024.org/people/reviewers/" target="_blank" style="color:#000000">BMVC 2024 <font color="#b80000">(rate: 19.3%, 166/860)</font></a></b>. Happy to contribute to BMVC!</li>
      <li class="news-item"><span class="news-date">[2024.11.04]</span>I was listed as an <i>Outstanding Reviewer</i> at <b><a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#000000">ACM MM 2024 <font color="#b80000">(139/x)</font></a></b>. Happy to contribute to ACM MM!</li>
      <li class="news-item"><span class="news-date">[2024.10.10]</span>One paper on <i>neural nets & optimizers</i>, <i><a href="https://arxiv.org/abs/2410.06373" target="_blank" style="color:#002FA7"><b>BOCB</b></a></i>, is released. Welcome to visit <a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">Hugging Face</a> and <a href="https://bocb-ai.github.io" target="_blank" style="color:#002FA7">Project</a> page.</li>
      <li class="news-item"><span class="news-date">[2024.09.24]</span>I was listed as an <i>Outstanding Reviewer</i> at <b><a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#000000">ECCV 2024 <font color="#b80000">(rate: 2.7%, 198/7293)</font></a></b>. Happy to contribute to ECCV!</li>
      <li class="news-item"><span class="news-date">[2024.04.16]</span>One co-authored paper on <i>sequence modeling</i>, <i><a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#002FA7"> <b>LongVQ</b></a></i>, is accepted at <b><a href="https://ijcai24.org" target="_blank" style="color:#000000">IJCAI 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#002FA7">Zicheng Liu</a>! </li>
      <li class="news-item"><span class="news-date">[2024.01.16]</span>One paper on <i>vision backbone</i>, <i><a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#002FA7"> <b>MogaNet</b></a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. <a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code & weights</a> (220 stars) are released!</li>
      <li class="news-item"><span class="news-date">[2024.01.16]</span>One co-authored paper on <i>semi-sup learning</i>, <i><a href="https://arxiv.org/abs/2310.03013" target="_blank" style="color:#002FA7"> <b>SemiReward</b></a></i>, is accepted at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>! </li>
      <li class="news-item"><span class="news-date">[2024.01.09]</span>I am invited to serve as an <i> emergency reviewer</i> at <b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR 2024</a></b> <b><a href="https://iclr.cc/Conferences/2024/CallForTinyPapers" target="_blank" style="color:#000000">(TinyPapers)</a></b>. It will be held in Vienna, Austria. </li>
      <li class="news-item"><span class="news-date">[2023.09.31]</span>One co-authored paper on <i>video prediction</i>, <i><a href="https://arxiv.org/abs/2306.11249" target="_blank" style="color:#002FA7"> <b>OpenSTL</b></a></i>, is accepted at <b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">NeurIPS 2023</a></b>. Congrats to <a href="https://scholar.google.com/citations?hl=en&user=6kTV6aMAAAAJ" target="_blank" style="color:#002FA7">Cheng Tan</a>! </li>
      <li class="news-item"><span class="news-date">[2023.06.25]</span>Got my B.Eng. degree from Huazhong University of Science and Technology! Special thanks to my undergraduate supervisor Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a> for his generous support!</li>
      <li class="news-item"><span class="news-date">[2022.09.11]</span>One preprint on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2209.04851" target="_blank" style="color:#002FA7"><b>OpenMixup</b></a></i>, is presented for vision tasks. This is also my first arXiv paper!</li>
      <li class="news-item"><span class="news-date">[2022.09.11]</span>Maintain an open-source repository, <i><a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#002FA7"><b>OpenMixup</b></a></i> (648 stars), for both supervised, semi- and self-supervised visual representation learning based on PyTorch. On updating! </li>
      <li class="news-item"><span class="news-date">[2022.07.06]</span>Fortunate to become <i>visiting student</i> at Westlake University. </li>
      <li class="news-item"><span class="news-date">[2021.09.01]</span>Fortunate to become <i>research intern</i> under the supervision of Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>. </li>
      <li class="news-item"><span class="news-date">[2021.06.01]</span>Fortunate to become <i>research intern</i> in <a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#002FA7"> SIAT-MMLab</a> at Shenzhen Institute of Advanced Technology, CAS. </li>
    </ul>
  </div>


<A NAME="Interests"><h2>Research Interests</h2></A>
Currently, I focus mostly on Computer Vision, Multi-task and Multi-modal Learning, including (but not limited to):
<ul>
    <li style="margin-bottom: 8px;"><strong>Data-efficient Learning:</strong> <a href="https://arxiv.org/abs/2209.04851" target="_blank" style="color:#002FA7"><b>OpenMixup</b> (arXiv'22)</a>, <a href="https://arxiv.org/abs/2111.15454" target="_blank" style="color:#002FA7"><b>SAMix</b> (arXiv'22)</a>, <a href="https://arxiv.org/abs/2310.03013" target="_blank" style="color:#002FA7"><b>SemiReward</b> (ICLR'24)</a></li>
    <li style="margin-bottom: 8px;"><strong>Efficient Model Architectures:</strong> <a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#002FA7"><b>MogaNet</b> (ICLR'24)</a>, <a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#002FA7"><b>MergeVQ</b> (CVPR'25)</a></li>
    <li style="margin-bottom: 8px;"><strong>(Multi-task) Optimizers:</strong> <a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7"><b>BOCB</b> (arXiv'24)</a>, <a href="https://huggingface.co/papers/2506.01049" target="_blank" style="color:#002FA7"><b>SGG</b> (ACL'25 Main)</a>, <a href="https://huggingface.co/papers/2506.01049" target="_blank" style="color:#002FA7"><b>Rep-MTL</b></a></li>
</ul>


<style>
    p[style*="margin-left: 1.9em;"],
    p[style*="margin-left: 1.7em;"],
    p[style*="margin-left: 1.2em;"],
    p[style*="margin-left: 1.0em;"] {
      border-radius: 8px;
      padding: 11px;
      margin-bottom: 20px;
      transition: all 0.3s ease;
    }
    
    
    
    
    p[style*="margin-left"] img:hover {
      transform: scale(1.05);
    }
    
</style>


<A NAME="Education"><h2>Education</h2></A>
<p style="line-height: 0.05;">&nbsp; </p>

<p style="margin-left: 1.9em;"> 
    <img src="Files/HKUST_logo.png" align="left" width="55" height="78"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://hkust.edu.hk" target="_blank" style="color:#000000">The Hong Kong University of Science and Technology</a> (2025-now) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; Ph.D. in Computer Science and Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Multi-task and Multi-modal Learning. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>.<br>
</p>

<p style="margin-left: 1.0em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="67"/>
    &nbsp&nbsp&nbsp&nbsp <b><a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank" style="color:#000000">Huazhong University of Science and Technology</a> (2019-2023) </b><br> 
    &nbsp&nbsp&nbsp&nbsp &bull; B.Eng. in Electronic and Information Engineering. <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Thesis:</i> Efficient ConvNet-based Vision Backbone for Multiple Tasks (Grade: 92/100). <br> 
    &nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.<br>
</p>

<p style="line-height: 0.05;">&nbsp; </p>
<A NAME="Internship"><h2>Research Experience</h2></A>
<p style="line-height: 0.05;">&nbsp; </p>

<p style="margin-left: 1.7em;"> 
    <img src="Files/ZEEKR_logo.png" align="left" width="65" height="65"/>
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://en.wikipedia.org/wiki/Zeekr" target="_blank" style="color:#000000">ZEEKR Intelligent Technology</a> (2024.04-2025.04) </b><br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Efficient Multi-task Optimization. <br> 
    &nbsp&nbsp&nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a> (University-Enterprise Cooperation). <br>
</p>


<p style="margin-left: 1.9em;"> 
    <img src="Files/westlake.ico" align="left" width="62" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b><a href="https://en.westlake.edu.cn" target="_blank" style="color:#000000">AI Lab, Westlake University</a> (2022.07-2024.02) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Topics:</i> Visual Representation Learning and AI for Science. <br> 
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &bull; <i>Advisor:</i> Chair Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Stan Z. Li</a> (IEEE Fellow, IAPR Fellow).<br>
</p>


<p style="margin-left: 1.2em;"> 
    <img src="Files/HUST_logo.jpg" align="left" width="86" height="65"/>
    &nbsp&thinsp;&thinsp;&thinsp; <b><a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#000000">Xinggang Wang's Vision Group, HUST</a> (2021.09-2023.03) </b><br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Topics:</i> Few-shot Semantic Segmentation. <br> 
    &nbsp&thinsp;&thinsp;&thinsp; &bull; <i>Advisor:</i> Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>.<br>
</p>


<p style="margin-left: 1.9em;"> 
    <img src="Files/SIAT_logo.png" align="left" width="64" height="65"/>
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; <b><a href="https://mmlab.siat.ac.cn" target="_blank" style="color:#000000">SIAT-MMLab, Chinese Academy of Sciences</a> (2021.06-2021.09) </b><br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Topics:</i> Semantic Segmentation and Text Spotting. <br> 
    &nbsp&nbsp&nbsp&nbsp&thinsp;&thinsp; &bull; <i>Advisor:</i> Dr. <a href="https://scholar.google.com/citations?hl=en&user=9WhK1y4AAAAJ" target="_blank" style="color:#002FA7">Bin Fu</a>.<br>
</p>

<p style="line-height: 0.05;">&nbsp; </p>


<A NAME="Awards"><h2>Selected Awards and Honors</h2></A>
<p style="line-height: 0.1;">&nbsp; </p>

<ul class="awards-grid">
    <li class="award-card">
        <div class="icon"><i class="fas fa-award"></i></div>
        <div>
            <div class="conference"><a href="https://iclr.cc/Conferences/2025/Reviewers" target="_blank" style="color:#000000">ICLR 2025</a></div>
            <div class="details">Notable Reviewer<br><font color="#b80000">(Top 2.6%)</font></div>
        </div>
    </li>
    <li class="award-card">
        <div class="icon"><i class="fas fa-award"></i></div>
        <div>
            <div class="conference"><a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers" target="_blank" style="color:#000000">ECCV 2024</a></div>
            <div class="details">Outstanding Reviewer<br><font color="#b80000">(Top 2.7%)</font></div>
        </div>
    </li>
    <li class="award-card">
        <div class="icon"><i class="fas fa-award"></i></div>
        <div>
            <div class="conference"><a href="https://2024.acmmm.org/outstanding-ac-reviewer" target="_blank" style="color:#000000">ACM MM 2024</a></div>
            <div class="details">Outstanding Reviewer<br><font color="#b80000">(139/x)</font></div>
        </div>
    </li>
    <li class="award-card">
        <div class="icon"><i class="fas fa-award"></i></div>
        <div>
            <div class="conference"><a href="https://bmvc2024.org/people/reviewers/" target="_blank" style="color:#000000">BMVC 2024</a></div>
            <div class="details">Outstanding Reviewer<br><font color="#b80000">(Top 19.3%)</font></div>
        </div>
    </li>
</ul>


<A NAME="Publications"><h2>Publications</h2></A>



<style>
.imgtable {
  margin-bottom: 25px;
  transition: all 0.3s ease;
  border-radius: 12px;
}

.imgtable:hover {
  background-color: #fafaff;
}

.pub-img {
    transition: transform 0.3s ease, box-shadow 0.3s ease;
    border: 1px solid #464646 !important;
    padding: 5px !important;
    border-radius: 12px !important;
    box-shadow: 2px 2px 10px rgba(0,0,0,0.1) !important;
}
    
.pub-img:hover {
  transform: scale(1.03);
  box-shadow: 0 12px 24px rgba(0,0,0,0.15) !important;
}

.pub-title {
    font-size: 15.5px; /* Slightly larger than normal text */
    line-height: 1.3;
    font-weight: bold;
    font-family: Georgia, serif;
    margin-bottom: 3px;
    color: #002FA7;
}
.imgtable td {
  padding: 12px;
  vertical-align: top;
}

.publications-list > ul {
  padding-left: 15px; /* Adjust this value if needed */
}
</style>




<p><b>Selected Preprints (*: Equal Contribution. â€ : Corresponding Author.)</b>: </p>
<font size="3" class="publications-list"> 
<ul>
<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_BOCB.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"> <a href= "https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning</a></div>
        <i> Siyuan Li*, Juanxi Tian*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang*</a></b>, Luyuan Zhang, Zicheng Liu, Weiyang Jin, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2410.06373" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2410.06373" target="_blank" style="color:#002FA7">ðŸ¤—HF</a>]
        [<a href="https://bocb-ai.github.io" target="_blank" style="color:#002FA7">Project</a>]
        [<a href="https://github.com/Black-Box-Optimization-Coupling-Bias/BOCB" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2024_SEMA.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" height="104px"/>&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href= "https://huggingface.co/papers/2402.09240" target="_blank" style="color:#002FA7">Switch EMA: A Free Lunch for Better Flatness and Sharpness</a></div>
        <i> Siyuan Li*, Zicheng Liu*, Juanxi Tian*, Ge Wang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang</a></b>, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2402.09240" target="_blank" style="color:#002FA7">ðŸ¤—HF</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>  

<table class="imgtable"><tr><td>
    <img src="./Teasers/arXiv_2023_MM_Survey.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#002FA7">Masked Modeling for Self-supervised Representation Learning on Vision and Beyond</a></div>
        <i> Siyuan Li*, Luyuan Zhang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang</a></b>, Di Wu, Lirong Wu, Zicheng Liu, Jun Xia, Cheng Tan, Yang Liu, Baigui Sun, Stan Z. Liâ€  </a></i><br><i><b>arXiv, 2023</b></i><br>
        [<a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Lupin1998/Awesome-MIM" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/arXiv_2023_MIMSurvey_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>



</ul>
<br />


<p><b>Conferences (As First Author)</b>: </p>
<font size="3" class="publications-list"> 
<ul>

<table class="imgtable"><tr><td>
  <img src="./Teasers/ACL25_SGG.pdf" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" height="122px"/>&nbsp;</td>
  <td align="left"><p>
      <div class="pub-title"><a href= "https://huggingface.co/papers/2506.01049" target="_blank" style="color:#002FA7">Taming LLMs by Scaling Learning Rates with Gradient Grouping</a></div>
      <i> Siyuan Li*, Juanxi Tian* <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang*</a></b>, Xin Jin, Zicheng Liu, Wentao Zhang, Dan Xu </a></i><br><i><b><a href="https://2025.aclweb.org" target="_blank" style="color:#000000">The 63rd Annual Meeting of the Association for Computational Linguistics (ACL Main), 2025</a></b></i><br>
      [<a href= "https://arxiv.org/abs/2506.01049" target="_blank" style="color:#002FA7">arXiv</a>]
      [<a href="https://huggingface.co/papers/2506.01049" target="_blank" style="color:#9F6821">ðŸ¤—<b>HF DailyPapers Top-5</b></a>]
      [<a href="https://scalingopt.github.io/SGG/" target="_blank" style="color:#002FA7">Project</a>]
      [<a href="./Files/acl_2025_SGG_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_MogaNet.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" height="120px"/>&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href= "https://iclr.cc/virtual/2024/poster/18447" target="_blank" style="color:#002FA7">MogaNet: Multi-order Gated Aggregation Network</a></div>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang*</a></b>, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Liâ€  </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2211.03295" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#002FA7">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/582542948" target="_blank" style="color:#002FA7">Zhihu</a>]
        [<a href="./Files/iclr_2024_MogaNet_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_VQDNA.jpeg" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href="https://proceedings.mlr.press/v235/li24bm.html" target="_blank" style="color:#002FA7">VQDNA: Unleashing the Power of Vector Quantization for Genomic Sequence Modeling</a></div>
        <i> Siyuan Li*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang*</a></b>, Zicheng Liu, Di Wu, Cheng Tan, Jiangbin Zheng, Yufei Huang, Stan Z. Liâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Lupin1998/VQDNA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/icml_2024_VQDNA_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

</ul>

<p><b>Conferences (As Co-author)</b>: </p>
<font size="3" class="publications-list"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Teasers/CVPR25_MergeVQ.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" height="120px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a target="_blank" style="color:#b80000">[NEW!] </a><a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#002FA7">MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization</a></div>
        <i>Siyuan Li*, Luyuan Zhang*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang</a></b>, Juanxi Tian, Cheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie, Haonan Lu, Haoqian Wang, Zhen Leiâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">IEEE/CVF Conference on Computer Vision (CVPR), 2025</a></b></i><br>
        [<a href="https://arxiv.org/abs/2504.00999" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#9F6821">ðŸ¤—<b>HF DailyPapers Top-1</b></a>]
        [<a href="https://apexgen-x.github.io/MergeVQ/" target="_blank" style="color:#002FA7">Project</a>]
        [<a href="./Files/cvpr_2025_MergeVQ_bibtex.html" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICML24_CHELA.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href="https://proceedings.mlr.press/v235/liu24ak.html" target="_blank" style="color:#002FA7">Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences</a></div>
        <i> Zicheng Liu, Siyuan Li, Li Wang, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang</a></b>, Yunfan Liu, Stan Z. Liâ€  </a></i><br><i><b><a href="https://icml.cc" target="_blank" style="color:#000000">International Conference on Machine Learning (ICML), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/icml_2024_CHELA_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/IJCAI24_LongVQ.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href="https://www.ijcai.org/proceedings/2024/510" target="_blank" style="color:#002FA7">LongVQ: Long Sequence Modeling with Vector Quantization on Structured Memory</a></div>
        <i> Zicheng Liu, Li Wang, Siyuan Li, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang</a></b>, Haitao Lin, Stan Z. Liâ€  </a></i><br><i><b><a href="https://ijcai24.org" target="_blank" style="color:#000000">International Joint Conference on Artificial Intelligence (IJCAI), 2024</a></b></i><br>
        [<a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/ijcai_2024_LongVQ_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>
    

<table class="imgtable"><tr><td>
    <img src="./Teasers/ICLR24_SemiReward.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href= "https://iclr.cc/virtual/2024/poster/18245" target="_blank" style="color:#002FA7">SemiReward: A General Reward Model for Semi-supervised Learning</a></div>
        <i> Siyuan Li*, Weiyang Jin*, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang</a></b>, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Liâ€  </a></i><br><i><b><a href="https://iclr.cc" target="_blank" style="color:#000000">International Conference on Learning Representations (ICLR), 2024</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2310.03013" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/Westlake-AI/SemiReward" target="_blank" style="color:#002FA7">Code</a>]
        [<a href="./Files/iclr_2024_SemiReward_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>


<table class="imgtable"><tr><td>
    <img src="./Teasers/NeurIPS23_OpenSTL.png" class="pub-img" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <div class="pub-title"><a href= "https://papers.nips.cc/paper_files/paper/2023/hash/dcbff44d11130e75d09d3930411c23e1-Abstract-Datasets_and_Benchmarks.html" target="_blank" style="color:#002FA7">OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning</a></div>
        <i> Cheng Tan*, Siyuan Li*, Zhangyang Gao, Wenfei Guan, <b><a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" class="dashed-underline-me">Zedong Wang</a></b>, Zicheng Liu, Lirong Wu, Stan Z. Liâ€  </a></i><br><i><b><a href="https://neurips.cc/Conferences/2023" target="_blank" style="color:#000000">Advances in Neural Information Processing Systems (NeurIPS), 2023</a></b></i><br>
        [<a href= "https://arxiv.org/abs/2306.11249" target="_blank" style="color:#002FA7">arXiv</a>]
        [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#002FA7">Code</a>]
        [<a href= "https://zhuanlan.zhihu.com/p/640271275" target="_blank" style="color:#002FA7">Zhihu</a>]
        [<a href="./Files/NIPS_2023_OpenSTL_bibtex" target="_blank" style="color:#002FA7">BibTeX</a>]
</p></td></tr></table>   


</ul>


<A NAME="Services"><h2>Professional Services</h2></A>

<p><b>Conference Reviewer/PC Member:</b> </p>
<font size="3"> 
<ul>

<li>International Conference on Learning Representations (<b><a href="https://iclr.cc" target="_blank" style="color:#000000">ICLR</a></b>), 2024 (TinyPapers), 2025<br /></li>
<li>Annual Conference on Neural Information Processing Systems (<b><a href="https://neurips.cc" target="_blank" style="color:#000000">NeurIPS</a></b>), 2024, 2025<br /></li>
<li>International Conference on Machine Learning (<b><a href="https://icml.cc" target="_blank" style="color:#000000">ICML</a></b>), 2024, 2025<br /></li>
<li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b><a href="https://cvpr.thecvf.com" target="_blank" style="color:#000000">CVPR</a></b>), 2025<br /></li>
<li>European Conference on Computer Vision (<b><a href="https://eccv.ecva.net" target="_blank" style="color:#000000">ECCV</a></b>), 2024<br /></li>
<li>AAAI Conference on Artificial Intelligence (<b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank" style="color:#000000">AAAI</a></b>), 2025<br /></li>
<li>ACM International Conference on Multimedia (<b><a href="https://2024.acmmm.org" target="_blank" style="color:#000000">ACM MM</a></b>), 2024<br /></li>
<li>BMVA The British Machine Vision Conference (<b><a href="http://bmvc2024.org" target="_blank" style="color:#000000">BMVC</a></b>), 2024, 2025<br /></li>
<li>IAPR International Conference on Pattern Recognition (<b><a href="https://icpr2024.org" target="_blank" style="color:#000000">ICPR</a></b>), 2024<br /></li>
</ul>
</font>
<br />
 
<p><b>Journal Reviewer:</b> </p>
<font size="3"> 
<ul>
<li>IEEE Transactions on Knowledge and Data Engineering (<b><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69" target="_blank" style="color:#000000">TKDE</a></b>)</li>
</ul>
</font>
<br />

<p><b>Membership</b>: </p>
<font size="3"> 
<ul>
<li>China Computer Federation (<b><a href="https://www.ccf.org.cn/en/" target="_blank" style="color:#000000">CCF</a></b>), Student Member, 2024-2026</li>
<li>China Society of Image and Graphics (<b><a href="https://en.csig.org.cn" target="_blank" style="color:#000000">CSIG</a></b>), Student Member, 2023</li>
</ul>
</font>
<br />




<A NAME="Acknowledgement"><h2>Acknowledgement</h2></A>
My research career cannot be possible without the support from all my awesome mentors, collaborators, and friends:
<ul>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a>, Prof. <a href="https://yuzhou.vlrlab.net" target="_blank" style="color:#002FA7">Yu Zhou</a> at HUST.</li>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#002FA7">Stan Z. Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=SKTQTXwAAAAJ" target="_blank" style="color:#002FA7">Siyuan Li</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=EwMGZsgAAAAJ" target="_blank" style="color:#002FA7">Zicheng Liu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=o5A23qIAAAAJ" target="_blank" style="color:#002FA7">Haitao Lin</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=egz8bGQAAAAJ" target="_blank" style="color:#002FA7">Jiangbin Zheng</a>, Mr. Siqi Ma at Westlake University.</li>
<li>Prof. <a href="https://scholar.google.com/citations?hl=en&user=OuSPv-AAAAAJ" target="_blank" style="color:#002FA7">Dan Xu</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=ennCRJAAAAAJ" target="_blank" style="color:#002FA7">Zhenxing Mi</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=4OuUHYQAAAAJ" target="_blank" style="color:#002FA7">Yuxin Wang</a>, Mr. <a href="https://wangandyyucheng.github.io" target="_blank" style="color:#002FA7">Yucheng Wang</a>, Mr. <a href="https://scholar.google.com/citations?hl=en&user=1xsk9rYAAAAJ" target="_blank" style="color:#002FA7">Yiwei Chen</a> at HKUST.</li>
<li>Mr. <a href="https://scholar.google.com/citations?hl=en&user=Ww37Oi4AAAAJ" target="_blank" style="color:#002FA7">Aozhong Zhang</a> at SUNY Albany.</li>
</ul>
<div style="text-align:justify">
<p style="line-height: 0.2;">&nbsp; </p>
Beyond academia, I feel incredibly fortunate to met wonderful friends along the way (particularly during my middle and high school years in Shenzhen and two years in Hangzhou). Special thanks to Chloe, we met by chance and both joined Westlake University for summer research in 2022. Thank you for your presence and support these years, and I am deeply grateful for our shared moments. Whether we are still in touch or not, I wish you all the best and a bright future!

<p style="line-height: 0.1;">&nbsp; </p>
Even if most of you above may never visit this page, my door is always open for a chat - coffee's on me! 
</div>
<br />



<A NAME="Fragments of Memories"><h2>Fragments of Memories</h2></A>
Great memories with my advisors: Prof. <a href="https://scholar.google.com/citations?hl=en&user=qNCTLV0AAAAJ" target="_blank" style="color:#002FA7">Xinggang Wang</a> (HUST). 
</p>

<img src="./Files/Zedong_With_Prof_Xinggang.jpeg" style="border:0.4px solid #464646;padding:5px;border-radius:14px;box-shadow:1.6px 1.6px #bbbbbb" alt="" width="190px" />&nbsp;</td>


<br />

<A NAME=""><h2></h2></A>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=305&t=n&d=p1SE8mvpEDXWAl_SKZ3NCPEvAjvDm5pAjUQB-nDQa5U&cmo=edbf4a&cmn=ff5f5f&co=306285'></script>

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<style>
    #back_top {
      position: fixed;
      bottom: 30px;
      right: 30px;
      width: 40px;
      height: 40px;
      background-color: #002FA7;
      border-radius: 50%;
      color: white;
      display: none;
      justify-content: center;
      align-items: center;
      cursor: pointer;
      box-shadow: 0 2px 10px rgba(0,0,0,0.2);
      transition: all 0.3s ease;
    }
    
    #back_top:hover {
      background-color: #001F6D;
      transform: translateY(-3px);
    }
    
    #back_top:before {
      content: "â–²";
      font-size: 18px;
    }
    </style>
    
    <!-- Replace the existing back_top div with: -->
    <div id="back_top"></div>
    
    <script>
    $(function(){
        $(window).scroll(function(){
            var scrollt = document.documentElement.scrollTop + document.body.scrollTop;
            if(scrollt > 400) {
                $("#back_top").fadeIn(400).css("display", "flex");
            } else {
                $("#back_top").stop().fadeOut(400);
            }
        });
    
        $("#back_top").click(function(){ 
            $("html,body").animate({scrollTop:"0px"}, 200);
        }); 
    });
    </script>

<div style="text-align:center; margin-top:20px; color:#A0A0A0; font-size:12px;">
    Last Updated: <span id="lastUpdated"></span>
  </div>
  
  <script>
    document.getElementById('lastUpdated').textContent = new Date().toLocaleDateString('en-US', { 
      year: 'numeric', 
      month: 'long', 
      day: 'numeric' 
    });
</script>

<!-- <font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2022.06.01</p>
</font> -->
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;All Rights Reserved by Zedong Wang. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.

</body>
</html>
